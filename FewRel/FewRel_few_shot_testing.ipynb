{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FewRel few-shot relation extraction\n",
    "\n",
    "* goal is to make it like the demo at http://opennre.thunlp.ai/#/fewshot_re - in the demo it seems to work pretty well\n",
    "* based on https://github.com/thunlp/FewRel\n",
    "* use torch 1.3.1\n",
    "* copy val_wiki.json as test_wiki.json in the data folder to make it work\n",
    "* python requirements same as opennre\n",
    "* the checkpoint files are very big, so they aren't in the repository. One checkpoint is at https://drive.google.com/file/d/1yiz3q3xNz-llsY55g5OdodxiH1RThYuz/view?usp=sharing\n",
    "* can't train on prof song's gpus, not enough vram. the hpc computers have enough vram, but i couldn't get pytorch to work on them, it uses a 10 year old version of linux.... \n",
    "* however testing using this code does actually work on the gpu (but you would need to use cuda pytorch (refer to pytorch.org) and whereever you are using model do \"model = model.cuda()\", and also \"tensor=tensor.cuda()\"). refer to test_script.py\n",
    "* on my laptop cpu this code takes about 2 seconds per query, on the gpu it's a lot faster, runs in under 1 second. the speed seems fine, but more testing is required to really find out. \n",
    "* i noticed that after a lot of training the model seems to overfit, so it doesn't work properly on the testing data. therefore we should probably make our own benchmark in order to compare different models to each other. \n",
    "* the maximum length is in terms of the number of bert tokens, not in terms of the number of characters in the sentence. increasing it seems to work fine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/pair-bert-train_wiki-val_wiki-5-1.pth.tar\"\n",
    "bert_pretrained_checkpoint = 'bert-base-uncased'\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0121 17:05:40.898946 140734844052928 file_utils.py:35] PyTorch version 1.3.1 available.\n",
      "/Users/GuruSenthil/Documents/ProgrammingProjects/FYP_19_20_Project/FYP-19-20/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/GuruSenthil/Documents/ProgrammingProjects/FYP_19_20_Project/FYP-19-20/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/GuruSenthil/Documents/ProgrammingProjects/FYP_19_20_Project/FYP-19-20/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/GuruSenthil/Documents/ProgrammingProjects/FYP_19_20_Project/FYP-19-20/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/GuruSenthil/Documents/ProgrammingProjects/FYP_19_20_Project/FYP-19-20/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/GuruSenthil/Documents/ProgrammingProjects/FYP_19_20_Project/FYP-19-20/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0121 17:05:43.081158 140734844052928 __init__.py:23] Loading model from /Users/GuruSenthil/.neuralcoref_cache/neuralcoref\n"
     ]
    }
   ],
   "source": [
    "from fewshot_re_kit.data_loader import FewRelDatasetPair, get_loader_pair\n",
    "from fewshot_re_kit.framework import FewShotREFramework\n",
    "from fewshot_re_kit.sentence_encoder import BERTPAIRSentenceEncoder\n",
    "from models.pair import Pair\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import neuralcoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = BERTPAIRSentenceEncoder(\n",
    "                    bert_pretrained_checkpoint,\n",
    "                    max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meow_loader = get_loader_pair('val_wiki', sentence_encoder,\n",
    "#                 N=5, K=1, Q=1, na_rate=0, batch_size=1, encoder_name='bert')\n",
    "\n",
    "val_data_loader = iter(FewRelDatasetPair('val_wiki', sentence_encoder, N=5, K=1, Q=1, na_rate=0, root='./data', encoder_name='bert'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pair(sentence_encoder, hidden_size=768)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(next(val_data_loader)[0]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load_model__(ckpt):\n",
    "    '''\n",
    "    ckpt: Path of the checkpoint\n",
    "    return: Checkpoint dict\n",
    "    '''\n",
    "    if os.path.isfile(ckpt):\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        print(\"Successfully loaded checkpoint '%s'\" % ckpt)\n",
    "        return checkpoint\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found at '%s'\" % ckpt)\n",
    "\n",
    "        \n",
    "def item(x):\n",
    "    '''\n",
    "    PyTorch before and after 0.4\n",
    "    '''\n",
    "    torch_version = torch.__version__.split('.')\n",
    "    if int(torch_version[0]) == 0 and int(torch_version[1]) < 4:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x.item()\n",
    "    \n",
    "def bert_tokenize(tokens, head_indices, tail_indices):\n",
    "    word = sentence_encoder.tokenize(tokens,\n",
    "            head_indices,\n",
    "            tail_indices)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "list(map(str, tokenizer(\"\"\"hello meow. meow is donald trump's friend\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from the model checkpoint state\n",
    "\n",
    "model.eval()\n",
    "state_dict = __load_model__(checkpoint_path)['state_dict']\n",
    "own_state = model.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    if name not in own_state:\n",
    "        continue\n",
    "    own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on the wikidata dataset, which is what they have already implemented.\n",
    "\n",
    "N = 5\n",
    "K = 1\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "with torch.no_grad():\n",
    "    for it in range(10):\n",
    "        batch, label = next(val_data_loader)\n",
    "        label = torch.tensor(label)\n",
    "        batch['word'] = torch.stack(batch['word'])\n",
    "        batch['seg'] = torch.stack(batch['seg'])\n",
    "        batch['mask'] = torch.stack(batch['mask'])\n",
    "        logits, pred = model(batch, N, K, Q * N + Q * na_rate)\n",
    "        print(pred, label)\n",
    "        right = model.accuracy(pred, label)\n",
    "        print(item(right.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "K = 2\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "#names have to be upper case, otherwise they are not detected by NER\n",
    "example_relation_data = [\n",
    "    {'name':'love',\n",
    "    'examples':[\n",
    "        {'sentence':'Meow loves Mo', 'head':'Meow', 'tail':'Mo'},\n",
    "        {'sentence':'Tom is in love with Jull', 'head':'Tom', 'tail':'Jull'}\n",
    "    ]},\n",
    "    {'name':'hate',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump hates the Mooch', 'head':'Trump', 'tail':'Mooch'},\n",
    "        {'sentence':'Ivanka and Jared dislike each other intensely', 'head':'Ivanka', 'tail':'Jared'}\n",
    "    ]},\n",
    "    {'name':'spouse',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump is married to Ivanka', 'head':'Trump', 'tail':'Ivanka'},\n",
    "        {'sentence':\"Bill went out with his wife Jill on saturday\", 'head':'Bill', 'tail':'Jill'}\n",
    "    ]},\n",
    "        {'name':'insult',\n",
    "    'examples':[\n",
    "        {'sentence':'The President said that Michael Cohen is a rat', 'head':'The President', 'tail':'Michael'},\n",
    "        {'sentence':'Meow and Tom threw jabs at each other', 'head':'Meow', 'tail':'Tom'}\n",
    "    ]},\n",
    "        {'name':'capital',\n",
    "    'examples':[\n",
    "        {'sentence':'Austin is the capital of Texas', 'head':'Austin', 'tail':'Texas'},\n",
    "        {'sentence':\"the capital of China is located in Beijing\", 'head':'Beijing', 'tail':\"China\"}\n",
    "    ]}\n",
    "    \n",
    "]\n",
    "\n",
    "queries = [{\n",
    "    'sentence':'Cohen and Fluffy are very loving to each other','head':'Cohen','tail':'Fluffy'\n",
    "},\n",
    "{\n",
    "    'sentence':\"\"\"The US's capital is Washington\"\"\",'head':'Washington','tail':'US'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using already specified entities in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    return list(map(str, nlp(sentence)))\n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "#     tokens = q['sentence'].split(\" \")  #TODO: generalize, make it tokenize like in the example wikidata, would probably need to use some nlp library to do it\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    tokenized_head = spacy_tokenize(q['head'])\n",
    "    tokenized_tail = spacy_tokenize(q['tail'])\n",
    "    head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   #TODO: make it work with multi-word entities\n",
    "    tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "    bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "    for relation in example_relation_data:\n",
    "        for ex in relation['examples']:\n",
    "#             tokens = ex['sentence'].split(\" \")  #TODO: generalize\n",
    "            tokens = spacy_tokenize(ex['sentence'])\n",
    "            tokenized_head = spacy_tokenize(ex['head'])\n",
    "            tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "            head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "            tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "            bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "            \n",
    "            SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "            word_tensor = torch.zeros((max_length)).long()\n",
    "            \n",
    "            new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "            for i in range(min(max_length, len(new_word))):\n",
    "                word_tensor[i] = new_word[i]\n",
    "            mask_tensor = torch.zeros((max_length)).long()\n",
    "            mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "            seg_tensor = torch.ones((max_length)).long()\n",
    "            seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "            fusion_set['word'].append(word_tensor)\n",
    "            fusion_set['mask'].append(mask_tensor)\n",
    "            fusion_set['seg'].append(seg_tensor)\n",
    "    \n",
    "    fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "    fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "    fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "    logits, pred = model(fusion_set, N, K, 1)\n",
    "    print(pred, logits)\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# ex = \"hello Meow. Meow is Donald Trump's friend\"\n",
    "ex = \"\"\"The US's capital is Washington\"\"\"\n",
    "doc = nlp(ex)\n",
    "print(doc._.coref_resolved)\n",
    "doc = nlp(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(str, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using entities detected in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, nlp(doc._.coref_resolved)))\n",
    "\n",
    "def get_head_tail_pairs(sentence):\n",
    "    acceptable_entity_types = ['PERSON', 'NORP', 'ORG', 'GPE', 'PRODUCT', 'EVENT', 'LAW', 'LOC', 'FAC']\n",
    "    doc = nlp(sentence)\n",
    "    doc = nlp(doc._.coref_resolved)\n",
    "    entity_info = [(X.text, X.label_) for X in doc.ents]\n",
    "    entity_info = set(map(lambda x:x[0], filter(lambda x:x[1] in acceptable_entity_types, entity_info)))\n",
    "\n",
    "    return combinations(entity_info, 2)\n",
    "    \n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters - by default it was 128, seems to work with longer lengths also though.\n",
    "# the actual length of the sentence doesn't matter, only the number of bert tokens which are created. and this is managed automatically. \n",
    "\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    \n",
    "    \n",
    "    for head, tail in get_head_tail_pairs(q['sentence']):  #iterating through all possible combinations of 2 named entities\n",
    "        tokenized_head = spacy_tokenize(head)\n",
    "        tokenized_tail = spacy_tokenize(tail)\n",
    "        head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   \n",
    "        tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "        bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "        for relation in example_relation_data:\n",
    "            for ex in relation['examples']:\n",
    "                tokens = spacy_tokenize(ex['sentence'])\n",
    "                tokenized_head = spacy_tokenize(ex['head'])  #head and tail spelling and punctuation should match the corefered output exactly\n",
    "                tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "                head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "                tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "                bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "\n",
    "                SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "                CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "                word_tensor = torch.zeros((max_length)).long()\n",
    "\n",
    "                new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "                for i in range(min(max_length, len(new_word))):\n",
    "                    word_tensor[i] = new_word[i]\n",
    "                mask_tensor = torch.zeros((max_length)).long()\n",
    "                mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "                seg_tensor = torch.ones((max_length)).long()\n",
    "                seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "                fusion_set['word'].append(word_tensor)\n",
    "                fusion_set['mask'].append(mask_tensor)\n",
    "                fusion_set['seg'].append(seg_tensor)\n",
    "\n",
    "        fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "        fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "        fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            fusion_set['word'] = fusion_set['word'].cuda()\n",
    "            fusion_set['seg'] = fusion_set['seg'].cuda()\n",
    "            fusion_set['mask'] = fusion_set['mask'].cuda()\n",
    "        \n",
    "        logits, pred = model(fusion_set, N, K, 1)\n",
    "        print('Sentence: \\\"{}\\\", head: \\\"{}\\\", tail: \\\"{}\\\", prediction: {}'.format(q['sentence'], head, tail, example_relation_data[pred.item()]['name']))   #TODO: handle na case, which would be out of bounds\n",
    "    \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_functions import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Detector()\n",
    "d.run_on_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Mueller connections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "#this code can be used to go through the dataset and make sure that the heads and tails are accurate.\n",
    "\n",
    "#use utf 8 encoding if there are errors. can open the csv in sublime and resave it to use utf 8 encoding.\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")   #this csv file is heavily modified from the original, it has been cleaned to make sure that the heads and tails actually exist.\n",
    "#it could be cleaned even further by getting rid of useless relations etc.\n",
    "dfi = df[['sentence', 'head', 'tail', 'reldescription']].copy()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")    #no coref being done here, the assumption is that no coref will be done on the support/training data, only on test data.\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, doc))\n",
    "\n",
    "for ind, row in dfi.iterrows():\n",
    "#     if ind < 304:\n",
    "#         continue\n",
    "    head = row['head']\n",
    "    tail = row['tail']\n",
    "    sentence = row['sentence']\n",
    "    \n",
    "    tokens = spacy_tokenize(sentence)\n",
    "    \n",
    "#     print(ind)\n",
    "    tokenized_head = spacy_tokenize(head)\n",
    "    tokenized_tail = spacy_tokenize(tail)\n",
    "    \n",
    "    head_indices = None\n",
    "    tail_indices = None\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_head[0] and tokens[i:i+len(tokenized_head)] == tokenized_head:\n",
    "            head_indices = list(range(i,i+len(tokenized_head)))\n",
    "            break\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_tail[0] and tokens[i:i+len(tokenized_tail)] == tokenized_tail:\n",
    "            tail_indices = list(range(i,i+len(tokenized_tail)))\n",
    "            break\n",
    "    if head_indices is None or tail_indices is None:\n",
    "        print(sentence)\n",
    "        print(head)\n",
    "        print(tail)\n",
    "        raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
