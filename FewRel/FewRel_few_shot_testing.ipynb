{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FewRel few-shot relation extraction\n",
    "\n",
    "* goal is to make it like the demo at http://opennre.thunlp.ai/#/fewshot_re - in the demo it seems to work pretty well\n",
    "* based on https://github.com/thunlp/FewRel\n",
    "* use torch 1.3.1\n",
    "* copy val_wiki.json as test_wiki.json in the data folder to make it work\n",
    "* python requirements same as opennre. use the requirements.txt from there.\n",
    "* the checkpoint files are very big, so they aren't in the repository. (5-way 1-shot) checkpoint is at https://drive.google.com/file/d/1yiz3q3xNz-llsY55g5OdodxiH1RThYuz/view?usp=sharing\n",
    "* 5-way 3-shot checkpoint: https://drive.google.com/open?id=1mzeMyu4yjcLXWSi1CjFEvsrtE6EaAqv7\n",
    "* 5-way 3-shot with N/A checkpoint: https://drive.google.com/open?id=1C9tn_vpFf4tDcopZTZwBN2lwnqc3sKtF\n",
    "* can't train on prof song's gpus, not enough vram. the hpc computers have enough vram, but i couldn't get pytorch to work on them, it uses a 10 year old version of linux.... \n",
    "* however testing using this code does actually work on the gpu (but you would need to use cuda pytorch (refer to pytorch.org) and whereever you are using model do \"model = model.cuda()\", and also \"tensor=tensor.cuda()\"). refer to test_script.py\n",
    "* on my laptop cpu this code takes about 2 seconds per query, on the gpu it's a lot faster, runs in under 1 second. the speed seems fine, but if there are a lot of queries it will take a decent amount of time to run.\n",
    "* we should probably make our own benchmark in order to compare different models to each other. \n",
    "* the maximum length is in terms of the number of bert tokens, not in terms of the number of characters in the sentence. increasing it seems to work fine\n",
    "* running in macos seems to not be optimized correctly in pytorch, it makes the laptop basically unusable. *I highly recommend that you only run this code on the server. you can follow the directions in the knowcomp manual to make jupyter lab work correctly from the server*\n",
    "* I investigated if there would be any issues because the maximum number of bert tokens allowed is 128. I don't think that there would be many problems due to this. From my testing, the 128 token limit is only reached with extremely long sentences, which are definitely not the norm anyway.\n",
    "* make sure that there are no spaces in unexpected places, because they can cause problems since I am assuming that the strings match up exactly always.\n",
    "* also you should only pass 1 sentence at a time.\n",
    "* tried out the 5-1 and 5-3-na3 models on the more realistic dataset - the 5-3 and 5-3-na3 models are the ones which work the best for sure. 5-3-na3 may be slightly better than the other one imo. at this point both seem to have only around 50% accuracies.\n",
    "* which entity is the head and which one is the tail definitely matters, the results are different if they are swapped. The softmax results can also be significantly different. I'm not sure if this is due to how the model was trained or due to the support dataset.\n",
    "* also if there are multiple relations in the same sentence additional processing will have to be done to figure out the actual results. but at least in this model you have a chance to actually figure out the different relations, if you use a classifier model you definitely won't be able to.\n",
    "* the choice of support sentences definitely seems to have a significant effect on the output of the model. simpler sentences seem to work better.\n",
    "* also which words are used as the head and the tail probably matter. one problem with the spacy ner model is that it doesn't detect long phrases as entities. ex. in \"Trump had opposed President Obama's invasion of Iraq\", i think i would want the entities detected to be \"Trump\" and \"President Obama's invasion of Iraq\", but instead the model detects \"Trump\", \"Obama\" and \"Iraq\" - this probably affects the accuracy of the output. I could try to test this.....\n",
    "* need to test how much things like tenses and stuff affect the output. \n",
    "* i was actually accidently running it on the cpu on the server, it runs pretty fast on the cpu, like a quarter of a second per prediction. it doesn't actually seem to run much faster on the gpu. However, when running in this jupyter notebook the gpu sometimes runs  out of memory.... which is very weird.... this didn't happen when using the script for some reason. it outputs some predictions, then it runs out of memory and crashes.... i guess we'll just have to use the cpu.\n",
    "* with 4-shot rather than 3-shot it seems to work better. with 5-shot it seems to work about the same as 4-shot. the na3 model definitely seems to work better with 5-shot.\n",
    "* noticed that using the exact same word as in the relation support dataset doesn't seem to matter much, synonyms seem to produce pretty much the same output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/pair-bert-train_wiki-val_wiki-5-1.pth.tar\"\n",
    "bert_pretrained_checkpoint = 'bert-base-uncased'\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewshot_re_kit.data_loader import FewRelDatasetPair, get_loader_pair\n",
    "from fewshot_re_kit.framework import FewShotREFramework\n",
    "from fewshot_re_kit.sentence_encoder import BERTPAIRSentenceEncoder\n",
    "from models.pair import Pair\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import neuralcoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = BERTPAIRSentenceEncoder(\n",
    "                    bert_pretrained_checkpoint,\n",
    "                    max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meow_loader = get_loader_pair('val_wiki', sentence_encoder,\n",
    "#                 N=5, K=1, Q=1, na_rate=0, batch_size=1, encoder_name='bert')\n",
    "\n",
    "val_data_loader = iter(FewRelDatasetPair('val_wiki', sentence_encoder, N=5, K=1, Q=1, na_rate=0, root='./data', encoder_name='bert'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pair(sentence_encoder, hidden_size=768)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(next(val_data_loader)[0]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load_model__(ckpt):\n",
    "    '''\n",
    "    ckpt: Path of the checkpoint\n",
    "    return: Checkpoint dict\n",
    "    '''\n",
    "    if os.path.isfile(ckpt):\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        print(\"Successfully loaded checkpoint '%s'\" % ckpt)\n",
    "        return checkpoint\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found at '%s'\" % ckpt)\n",
    "\n",
    "        \n",
    "def item(x):\n",
    "    '''\n",
    "    PyTorch before and after 0.4\n",
    "    '''\n",
    "    torch_version = torch.__version__.split('.')\n",
    "    if int(torch_version[0]) == 0 and int(torch_version[1]) < 4:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x.item()\n",
    "    \n",
    "def bert_tokenize(tokens, head_indices, tail_indices):\n",
    "    word = sentence_encoder.tokenize(tokens,\n",
    "            head_indices,\n",
    "            tail_indices)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "list(map(str, tokenizer(\"\"\"hello meow. meow is donald trump's friend\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from the model checkpoint state\n",
    "\n",
    "model.eval()\n",
    "state_dict = __load_model__(checkpoint_path)['state_dict']\n",
    "own_state = model.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    if name not in own_state:\n",
    "        continue\n",
    "    own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on the wikidata dataset, which is what they have already implemented.\n",
    "\n",
    "N = 5\n",
    "K = 1\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "with torch.no_grad():\n",
    "    for it in range(10):\n",
    "        batch, label = next(val_data_loader)\n",
    "        label = torch.tensor(label)\n",
    "        batch['word'] = torch.stack(batch['word'])\n",
    "        batch['seg'] = torch.stack(batch['seg'])\n",
    "        batch['mask'] = torch.stack(batch['mask'])\n",
    "        logits, pred = model(batch, N, K, Q * N + Q * na_rate)\n",
    "        print(pred, label)\n",
    "        right = model.accuracy(pred, label)\n",
    "        print(item(right.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "K = 2\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "#names have to be upper case, otherwise they are not detected by NER\n",
    "example_relation_data = [\n",
    "    {'name':'love',\n",
    "    'examples':[\n",
    "        {'sentence':'Meow loves Mo', 'head':'Meow', 'tail':'Mo'},\n",
    "        {'sentence':'Tom is in love with Jull', 'head':'Tom', 'tail':'Jull'}\n",
    "    ]},\n",
    "    {'name':'hate',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump hates the Mooch', 'head':'Trump', 'tail':'Mooch'},\n",
    "        {'sentence':'Ivanka and Jared dislike each other intensely', 'head':'Ivanka', 'tail':'Jared'}\n",
    "    ]},\n",
    "    {'name':'spouse',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump is married to Ivanka', 'head':'Trump', 'tail':'Ivanka'},\n",
    "        {'sentence':\"Bill went out with his wife Jill on saturday\", 'head':'Bill', 'tail':'Jill'}\n",
    "    ]},\n",
    "        {'name':'insult',\n",
    "    'examples':[\n",
    "        {'sentence':'The President said that Michael Cohen is a rat', 'head':'The President', 'tail':'Michael'},\n",
    "        {'sentence':'Meow and Tom threw jabs at each other', 'head':'Meow', 'tail':'Tom'}\n",
    "    ]},\n",
    "        {'name':'capital',\n",
    "    'examples':[\n",
    "        {'sentence':'Austin is the capital of Texas', 'head':'Austin', 'tail':'Texas'},\n",
    "        {'sentence':\"the capital of China is located in Beijing\", 'head':'Beijing', 'tail':\"China\"}\n",
    "    ]}\n",
    "    \n",
    "]\n",
    "\n",
    "queries = [{\n",
    "    'sentence':'Cohen and Fluffy are very loving to each other','head':'Cohen','tail':'Fluffy'\n",
    "},\n",
    "{\n",
    "    'sentence':\"\"\"The US's capital is Washington\"\"\",'head':'Washington','tail':'US'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using already specified entities in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    return list(map(str, nlp(sentence)))\n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "#     tokens = q['sentence'].split(\" \")  #TODO: generalize, make it tokenize like in the example wikidata, would probably need to use some nlp library to do it\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    tokenized_head = spacy_tokenize(q['head'])\n",
    "    tokenized_tail = spacy_tokenize(q['tail'])\n",
    "    head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   #TODO: make it work with multi-word entities\n",
    "    tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "    bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "    for relation in example_relation_data:\n",
    "        for ex in relation['examples']:\n",
    "#             tokens = ex['sentence'].split(\" \")  #TODO: generalize\n",
    "            tokens = spacy_tokenize(ex['sentence'])\n",
    "            tokenized_head = spacy_tokenize(ex['head'])\n",
    "            tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "            head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "            tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "            bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "            \n",
    "            SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "            word_tensor = torch.zeros((max_length)).long()\n",
    "            \n",
    "            new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "            for i in range(min(max_length, len(new_word))):\n",
    "                word_tensor[i] = new_word[i]\n",
    "            mask_tensor = torch.zeros((max_length)).long()\n",
    "            mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "            seg_tensor = torch.ones((max_length)).long()\n",
    "            seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "            fusion_set['word'].append(word_tensor)\n",
    "            fusion_set['mask'].append(mask_tensor)\n",
    "            fusion_set['seg'].append(seg_tensor)\n",
    "    \n",
    "    fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "    fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "    fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "    logits, pred = model(fusion_set, N, K, 1)\n",
    "    print(pred, logits)\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# ex = \"hello Meow. Meow is Donald Trump's friend\"\n",
    "ex = \"\"\"The US's capital is Washington\"\"\"\n",
    "doc = nlp(ex)\n",
    "print(doc._.coref_resolved)\n",
    "doc = nlp(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(str, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using entities detected in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, nlp(doc._.coref_resolved)))\n",
    "\n",
    "def get_head_tail_pairs(sentence):\n",
    "    acceptable_entity_types = ['PERSON', 'NORP', 'ORG', 'GPE', 'PRODUCT', 'EVENT', 'LAW', 'LOC', 'FAC']\n",
    "    doc = nlp(sentence)\n",
    "    doc = nlp(doc._.coref_resolved)\n",
    "    entity_info = [(X.text, X.label_) for X in doc.ents]\n",
    "    entity_info = set(map(lambda x:x[0], filter(lambda x:x[1] in acceptable_entity_types, entity_info)))\n",
    "\n",
    "    return combinations(entity_info, 2)\n",
    "    \n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters - by default it was 128, seems to work with longer lengths also though.\n",
    "# the actual length of the sentence doesn't matter, only the number of bert tokens which are created. and this is managed automatically. \n",
    "\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    \n",
    "    \n",
    "    for head, tail in get_head_tail_pairs(q['sentence']):  #iterating through all possible combinations of 2 named entities\n",
    "        tokenized_head = spacy_tokenize(head)\n",
    "        tokenized_tail = spacy_tokenize(tail)\n",
    "        head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   \n",
    "        tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "        bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "        for relation in example_relation_data:\n",
    "            for ex in relation['examples']:\n",
    "                tokens = spacy_tokenize(ex['sentence'])\n",
    "                tokenized_head = spacy_tokenize(ex['head'])  #head and tail spelling and punctuation should match the corefered output exactly\n",
    "                tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "                head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "                tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "                bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "\n",
    "                SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "                CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "                word_tensor = torch.zeros((max_length)).long()\n",
    "\n",
    "                new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "                for i in range(min(max_length, len(new_word))):\n",
    "                    word_tensor[i] = new_word[i]\n",
    "                mask_tensor = torch.zeros((max_length)).long()\n",
    "                mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "                seg_tensor = torch.ones((max_length)).long()\n",
    "                seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "                fusion_set['word'].append(word_tensor)\n",
    "                fusion_set['mask'].append(mask_tensor)\n",
    "                fusion_set['seg'].append(seg_tensor)\n",
    "\n",
    "        fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "        fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "        fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            fusion_set['word'] = fusion_set['word'].cuda()\n",
    "            fusion_set['seg'] = fusion_set['seg'].cuda()\n",
    "            fusion_set['mask'] = fusion_set['mask'].cuda()\n",
    "        \n",
    "        logits, pred = model(fusion_set, N, K, 1)\n",
    "        print('Sentence: \\\"{}\\\", head: \\\"{}\\\", tail: \\\"{}\\\", prediction: {}'.format(q['sentence'], head, tail, example_relation_data[pred.item()]['name']))   #TODO: handle na case, which would be out of bounds\n",
    "    \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_functions import Detector\n",
    "d = Detector(chpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3.pth.tar\")\n",
    "d.run_on_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Mueller connections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code can be used to go through the dataset and make sure that the heads and tails are accurate.\n",
    "\n",
    "#use utf 8 encoding if there are errors. can open the csv in sublime and resave it to use utf 8 encoding.\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")   #this csv file is heavily modified from the original, it has been cleaned to make sure that the heads and tails actually exist.\n",
    "#it could be cleaned even further by getting rid of useless relations etc.\n",
    "dfi = df[['sentence', 'head', 'tail', 'reldescription']].copy()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")    #no coref being done here, the assumption is that no coref will be done on the support/training data, only on test data.\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, doc))\n",
    "\n",
    "for ind, row in dfi.iterrows():\n",
    "#     if ind < 304:\n",
    "#         continue\n",
    "    head = row['head']\n",
    "    tail = row['tail']\n",
    "    sentence = row['sentence']\n",
    "    \n",
    "    tokens = spacy_tokenize(sentence)\n",
    "    \n",
    "#     print(ind)\n",
    "    tokenized_head = spacy_tokenize(head)\n",
    "    tokenized_tail = spacy_tokenize(tail)\n",
    "    \n",
    "    head_indices = None\n",
    "    tail_indices = None\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_head[0] and tokens[i:i+len(tokenized_head)] == tokenized_head:\n",
    "            head_indices = list(range(i,i+len(tokenized_head)))\n",
    "            break\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_tail[0] and tokens[i:i+len(tokenized_tail)] == tokenized_tail:\n",
    "            tail_indices = list(range(i,i+len(tokenized_tail)))\n",
    "            break\n",
    "    if head_indices is None or tail_indices is None:\n",
    "        print(sentence)\n",
    "        print(head)\n",
    "        print(tail)\n",
    "        raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")\n",
    "df['reldescription'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = dfi['reldescription'].value_counts()\n",
    "cnt = cnt[cnt >= 3]\n",
    "dfi = dfi[dfi['reldescription'].isin(cnt.index)].copy().reset_index(drop=True)\n",
    "dft = dfi[dfi['reldescription'] == 'media platform']\n",
    "dft.sample(3, replace=True, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_framework import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader.load_relation_support_csv(\"connections_Mueller_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint 'checkpoint/pair-bert-train_wiki-val_wiki-5-3-na3.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "from fyp_detection_framework import DetectionFramework\n",
    "d = DetectionFramework(ckpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3-na3.pth.tar\")\n",
    "# d.load_support(\"connections_Mueller_cleaned.csv\", min_instance=9)\n",
    "d.load_support(\"test_relation_support_dataset.csv\", K=5)\n",
    "# d.load_queries(\"test_queries.csv\")\n",
    "d.load_queries_predefined_head_tail(\"test_queries_with_head_tail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meeting', 'oppose', 'hacking', 'assistance', 'funding']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(list(r['name'] for r in d.support))\n",
    "list(r['name'] for r in d.support)\n",
    "# d.support = [i for i in d.support if i['name'] != 'oppose']\n",
    "# d.support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Trump hacked Obama in 2017.\", head: \"Obama\", tail: \"Trump\", prediction: hacking\n",
      "Sentence: \"Trump had had contact, including a meeting in 2010, with Obama before he became President. \", head: \"Trump\", tail: \"Obama\", prediction: meeting\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Trump\", tail: \"Obama\", prediction: oppose\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Trump\", tail: \"Congress\", prediction: funding\n",
      "Sentence: \"Trump opposed President Obama's invasion of Iraq.\", head: \"Trump\", tail: \"invasion of Iraq\", prediction: NA\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Congress\", tail: \"Trump\", prediction: assistance\n",
      "Sentence: \"Trump opposed President Obama's invasion of Iraq.\", head: \"invasion of Iraq\", tail: \"Trump\", prediction: oppose\n",
      "Sentence: \"There was a meeting between the dean of HKUST and Carrie Lam, but it was inconclusive.\", head: \"dean of HKUST\", tail: \"Carrie Lam\", prediction: assistance\n",
      "Sentence: \"The professor had applied for funding from the college, but it was too little too late.\", head: \"The professor\", tail: \"the college\", prediction: funding\n",
      "Sentence: \"The US CDC offered to assist the Chinese government in solving the virus problem.\", head: \"US CDC\", tail: \"Chinese government\", prediction: funding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Trump hacked Obama in 2017.',\n",
       "  'Obama',\n",
       "  'Trump',\n",
       "  'hacking',\n",
       "  tensor([[[2.0054, 2.2795, 3.0414, 1.9942, 1.5606, 0.3336]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had had contact, including a meeting in 2010, with Obama before he became President. ',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'meeting',\n",
       "  tensor([[[ 1.7125,  1.6728, -0.8235,  0.5415, -0.9132,  0.5909]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'oppose',\n",
       "  tensor([[[1.8534, 2.6802, 0.5167, 1.5320, 0.2155, 0.3849]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Trump',\n",
       "  'Congress',\n",
       "  'funding',\n",
       "  tensor([[[0.7425, 0.4691, 0.6323, 1.7721, 2.6324, 0.3662]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump opposed President Obama's invasion of Iraq.\",\n",
       "  'Trump',\n",
       "  'invasion of Iraq',\n",
       "  'NA',\n",
       "  tensor([[[-0.7955,  0.2347, -2.5192, -2.2682, -3.2112,  0.8165]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Congress',\n",
       "  'Trump',\n",
       "  'assistance',\n",
       "  tensor([[[ 1.6256,  3.5395,  3.4252,  4.7709,  3.4471, -0.0958]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump opposed President Obama's invasion of Iraq.\",\n",
       "  'invasion of Iraq',\n",
       "  'Trump',\n",
       "  'oppose',\n",
       "  tensor([[[-0.5912,  1.7290,  1.1227,  0.3703, -2.8448,  0.8408]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('There was a meeting between the dean of HKUST and Carrie Lam, but it was inconclusive.',\n",
       "  'dean of HKUST',\n",
       "  'Carrie Lam',\n",
       "  'assistance',\n",
       "  tensor([[[1.0560, 1.6441, 0.9229, 2.7353, 1.4904, 0.4512]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('The professor had applied for funding from the college, but it was too little too late.',\n",
       "  'The professor',\n",
       "  'the college',\n",
       "  'funding',\n",
       "  tensor([[[0.6763, 0.2837, 1.4635, 1.8344, 3.8564, 0.0220]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('The US CDC offered to assist the Chinese government in solving the virus problem.',\n",
       "  'US CDC',\n",
       "  'Chinese government',\n",
       "  'funding',\n",
       "  tensor([[[ 0.0502, -0.1579,  2.6098,  4.2105,  4.6245, -0.0834]]],\n",
       "         grad_fn=<CatBackward>))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
