{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FewRel few-shot relation extraction\n",
    "\n",
    "* goal is to make it like the demo at http://opennre.thunlp.ai/#/fewshot_re - in the demo it seems to work pretty well\n",
    "* based on https://github.com/thunlp/FewRel\n",
    "* use torch 1.3.1\n",
    "* copy val_wiki.json as test_wiki.json in the data folder to make it work\n",
    "* python requirements same as opennre\n",
    "* the checkpoint files are very big, so they aren't in the repository. One checkpoint (5-way 1-shot) is at https://drive.google.com/file/d/1yiz3q3xNz-llsY55g5OdodxiH1RThYuz/view?usp=sharing\n",
    "* 5-way 3-shot checkpoint: https://drive.google.com/open?id=1mzeMyu4yjcLXWSi1CjFEvsrtE6EaAqv7\n",
    "* 5-way 3-shot with N/A checkpoint: https://drive.google.com/open?id=1C9tn_vpFf4tDcopZTZwBN2lwnqc3sKtF\n",
    "* can't train on prof song's gpus, not enough vram. the hpc computers have enough vram, but i couldn't get pytorch to work on them, it uses a 10 year old version of linux.... \n",
    "* however testing using this code does actually work on the gpu (but you would need to use cuda pytorch (refer to pytorch.org) and whereever you are using model do \"model = model.cuda()\", and also \"tensor=tensor.cuda()\"). refer to test_script.py\n",
    "* on my laptop cpu this code takes about 2 seconds per query, on the gpu it's a lot faster, runs in under 1 second. the speed seems fine, but if there are a lot of queries it will take a decent amount of time to run.\n",
    "* i noticed that after a lot of training the model seems to overfit, so it doesn't work properly on the testing data. therefore we should probably make our own benchmark in order to compare different models to each other. \n",
    "* the maximum length is in terms of the number of bert tokens, not in terms of the number of characters in the sentence. increasing it seems to work fine\n",
    "* running in macos seems to not be optimized correctly in pytorch, it makes the laptop basically unusable. *I highly recommend that you only run this code on the server. you can follow the directions in the knowcomp manual to make jupyter lab work correctly from the server*\n",
    "* I investigated if there would be any issues because the maximum number of bert tokens allowed is 128. I don't think that there would be many problems due to this. From my testing, the 128 token limit is only reached with extremely long sentences, which are definitely not the norm anyway.\n",
    "* make sure that there are no spaces in unexpected places, because they can cause problems since I am assuming that the strings match up exactly always.\n",
    "* also you should only pass 1 sentence at a time.\n",
    "* tried out the 5-1 and 5-3-na3 models on the more realistic dataset - the 5-3 model is definitely the one which works the best. the 5-3-na3 model seems to be prone to predicting the same thing all the time for some reason.\n",
    "* which entity is the head and which one is the tail definitely matters, the results are different if they are swapped. The softmax results can also be significantly different. I'm not sure if this is due to how the model was trained or due to the support dataset.\n",
    "* also if there are multiple relations in the same sentence additional processing will have to be done to figure out the actual results. but at least in this model you have a chance to actually figure out the different relations, if you use a classifier model you definitely won't be able to.\n",
    "* the choice of support sentences definitely seems to have a significant effect on the output of the model. simpler sentences seem to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/pair-bert-train_wiki-val_wiki-5-1.pth.tar\"\n",
    "bert_pretrained_checkpoint = 'bert-base-uncased'\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewshot_re_kit.data_loader import FewRelDatasetPair, get_loader_pair\n",
    "from fewshot_re_kit.framework import FewShotREFramework\n",
    "from fewshot_re_kit.sentence_encoder import BERTPAIRSentenceEncoder\n",
    "from models.pair import Pair\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import neuralcoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = BERTPAIRSentenceEncoder(\n",
    "                    bert_pretrained_checkpoint,\n",
    "                    max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meow_loader = get_loader_pair('val_wiki', sentence_encoder,\n",
    "#                 N=5, K=1, Q=1, na_rate=0, batch_size=1, encoder_name='bert')\n",
    "\n",
    "val_data_loader = iter(FewRelDatasetPair('val_wiki', sentence_encoder, N=5, K=1, Q=1, na_rate=0, root='./data', encoder_name='bert'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pair(sentence_encoder, hidden_size=768)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(next(val_data_loader)[0]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load_model__(ckpt):\n",
    "    '''\n",
    "    ckpt: Path of the checkpoint\n",
    "    return: Checkpoint dict\n",
    "    '''\n",
    "    if os.path.isfile(ckpt):\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        print(\"Successfully loaded checkpoint '%s'\" % ckpt)\n",
    "        return checkpoint\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found at '%s'\" % ckpt)\n",
    "\n",
    "        \n",
    "def item(x):\n",
    "    '''\n",
    "    PyTorch before and after 0.4\n",
    "    '''\n",
    "    torch_version = torch.__version__.split('.')\n",
    "    if int(torch_version[0]) == 0 and int(torch_version[1]) < 4:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x.item()\n",
    "    \n",
    "def bert_tokenize(tokens, head_indices, tail_indices):\n",
    "    word = sentence_encoder.tokenize(tokens,\n",
    "            head_indices,\n",
    "            tail_indices)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "list(map(str, tokenizer(\"\"\"hello meow. meow is donald trump's friend\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from the model checkpoint state\n",
    "\n",
    "model.eval()\n",
    "state_dict = __load_model__(checkpoint_path)['state_dict']\n",
    "own_state = model.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    if name not in own_state:\n",
    "        continue\n",
    "    own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on the wikidata dataset, which is what they have already implemented.\n",
    "\n",
    "N = 5\n",
    "K = 1\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "with torch.no_grad():\n",
    "    for it in range(10):\n",
    "        batch, label = next(val_data_loader)\n",
    "        label = torch.tensor(label)\n",
    "        batch['word'] = torch.stack(batch['word'])\n",
    "        batch['seg'] = torch.stack(batch['seg'])\n",
    "        batch['mask'] = torch.stack(batch['mask'])\n",
    "        logits, pred = model(batch, N, K, Q * N + Q * na_rate)\n",
    "        print(pred, label)\n",
    "        right = model.accuracy(pred, label)\n",
    "        print(item(right.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "K = 2\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "#names have to be upper case, otherwise they are not detected by NER\n",
    "example_relation_data = [\n",
    "    {'name':'love',\n",
    "    'examples':[\n",
    "        {'sentence':'Meow loves Mo', 'head':'Meow', 'tail':'Mo'},\n",
    "        {'sentence':'Tom is in love with Jull', 'head':'Tom', 'tail':'Jull'}\n",
    "    ]},\n",
    "    {'name':'hate',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump hates the Mooch', 'head':'Trump', 'tail':'Mooch'},\n",
    "        {'sentence':'Ivanka and Jared dislike each other intensely', 'head':'Ivanka', 'tail':'Jared'}\n",
    "    ]},\n",
    "    {'name':'spouse',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump is married to Ivanka', 'head':'Trump', 'tail':'Ivanka'},\n",
    "        {'sentence':\"Bill went out with his wife Jill on saturday\", 'head':'Bill', 'tail':'Jill'}\n",
    "    ]},\n",
    "        {'name':'insult',\n",
    "    'examples':[\n",
    "        {'sentence':'The President said that Michael Cohen is a rat', 'head':'The President', 'tail':'Michael'},\n",
    "        {'sentence':'Meow and Tom threw jabs at each other', 'head':'Meow', 'tail':'Tom'}\n",
    "    ]},\n",
    "        {'name':'capital',\n",
    "    'examples':[\n",
    "        {'sentence':'Austin is the capital of Texas', 'head':'Austin', 'tail':'Texas'},\n",
    "        {'sentence':\"the capital of China is located in Beijing\", 'head':'Beijing', 'tail':\"China\"}\n",
    "    ]}\n",
    "    \n",
    "]\n",
    "\n",
    "queries = [{\n",
    "    'sentence':'Cohen and Fluffy are very loving to each other','head':'Cohen','tail':'Fluffy'\n",
    "},\n",
    "{\n",
    "    'sentence':\"\"\"The US's capital is Washington\"\"\",'head':'Washington','tail':'US'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using already specified entities in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    return list(map(str, nlp(sentence)))\n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "#     tokens = q['sentence'].split(\" \")  #TODO: generalize, make it tokenize like in the example wikidata, would probably need to use some nlp library to do it\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    tokenized_head = spacy_tokenize(q['head'])\n",
    "    tokenized_tail = spacy_tokenize(q['tail'])\n",
    "    head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   #TODO: make it work with multi-word entities\n",
    "    tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "    bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "    for relation in example_relation_data:\n",
    "        for ex in relation['examples']:\n",
    "#             tokens = ex['sentence'].split(\" \")  #TODO: generalize\n",
    "            tokens = spacy_tokenize(ex['sentence'])\n",
    "            tokenized_head = spacy_tokenize(ex['head'])\n",
    "            tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "            head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "            tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "            bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "            \n",
    "            SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "            word_tensor = torch.zeros((max_length)).long()\n",
    "            \n",
    "            new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "            for i in range(min(max_length, len(new_word))):\n",
    "                word_tensor[i] = new_word[i]\n",
    "            mask_tensor = torch.zeros((max_length)).long()\n",
    "            mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "            seg_tensor = torch.ones((max_length)).long()\n",
    "            seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "            fusion_set['word'].append(word_tensor)\n",
    "            fusion_set['mask'].append(mask_tensor)\n",
    "            fusion_set['seg'].append(seg_tensor)\n",
    "    \n",
    "    fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "    fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "    fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "    logits, pred = model(fusion_set, N, K, 1)\n",
    "    print(pred, logits)\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# ex = \"hello Meow. Meow is Donald Trump's friend\"\n",
    "ex = \"\"\"The US's capital is Washington\"\"\"\n",
    "doc = nlp(ex)\n",
    "print(doc._.coref_resolved)\n",
    "doc = nlp(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(str, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using entities detected in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, nlp(doc._.coref_resolved)))\n",
    "\n",
    "def get_head_tail_pairs(sentence):\n",
    "    acceptable_entity_types = ['PERSON', 'NORP', 'ORG', 'GPE', 'PRODUCT', 'EVENT', 'LAW', 'LOC', 'FAC']\n",
    "    doc = nlp(sentence)\n",
    "    doc = nlp(doc._.coref_resolved)\n",
    "    entity_info = [(X.text, X.label_) for X in doc.ents]\n",
    "    entity_info = set(map(lambda x:x[0], filter(lambda x:x[1] in acceptable_entity_types, entity_info)))\n",
    "\n",
    "    return combinations(entity_info, 2)\n",
    "    \n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters - by default it was 128, seems to work with longer lengths also though.\n",
    "# the actual length of the sentence doesn't matter, only the number of bert tokens which are created. and this is managed automatically. \n",
    "\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    \n",
    "    \n",
    "    for head, tail in get_head_tail_pairs(q['sentence']):  #iterating through all possible combinations of 2 named entities\n",
    "        tokenized_head = spacy_tokenize(head)\n",
    "        tokenized_tail = spacy_tokenize(tail)\n",
    "        head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   \n",
    "        tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "        bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "        for relation in example_relation_data:\n",
    "            for ex in relation['examples']:\n",
    "                tokens = spacy_tokenize(ex['sentence'])\n",
    "                tokenized_head = spacy_tokenize(ex['head'])  #head and tail spelling and punctuation should match the corefered output exactly\n",
    "                tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "                head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "                tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "                bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "\n",
    "                SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "                CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "                word_tensor = torch.zeros((max_length)).long()\n",
    "\n",
    "                new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "                for i in range(min(max_length, len(new_word))):\n",
    "                    word_tensor[i] = new_word[i]\n",
    "                mask_tensor = torch.zeros((max_length)).long()\n",
    "                mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "                seg_tensor = torch.ones((max_length)).long()\n",
    "                seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "                fusion_set['word'].append(word_tensor)\n",
    "                fusion_set['mask'].append(mask_tensor)\n",
    "                fusion_set['seg'].append(seg_tensor)\n",
    "\n",
    "        fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "        fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "        fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            fusion_set['word'] = fusion_set['word'].cuda()\n",
    "            fusion_set['seg'] = fusion_set['seg'].cuda()\n",
    "            fusion_set['mask'] = fusion_set['mask'].cuda()\n",
    "        \n",
    "        logits, pred = model(fusion_set, N, K, 1)\n",
    "        print('Sentence: \\\"{}\\\", head: \\\"{}\\\", tail: \\\"{}\\\", prediction: {}'.format(q['sentence'], head, tail, example_relation_data[pred.item()]['name']))   #TODO: handle na case, which would be out of bounds\n",
    "    \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_functions import Detector\n",
    "d = Detector(chpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3.pth.tar\")\n",
    "d.run_on_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Mueller connections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code can be used to go through the dataset and make sure that the heads and tails are accurate.\n",
    "\n",
    "#use utf 8 encoding if there are errors. can open the csv in sublime and resave it to use utf 8 encoding.\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")   #this csv file is heavily modified from the original, it has been cleaned to make sure that the heads and tails actually exist.\n",
    "#it could be cleaned even further by getting rid of useless relations etc.\n",
    "dfi = df[['sentence', 'head', 'tail', 'reldescription']].copy()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")    #no coref being done here, the assumption is that no coref will be done on the support/training data, only on test data.\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, doc))\n",
    "\n",
    "for ind, row in dfi.iterrows():\n",
    "#     if ind < 304:\n",
    "#         continue\n",
    "    head = row['head']\n",
    "    tail = row['tail']\n",
    "    sentence = row['sentence']\n",
    "    \n",
    "    tokens = spacy_tokenize(sentence)\n",
    "    \n",
    "#     print(ind)\n",
    "    tokenized_head = spacy_tokenize(head)\n",
    "    tokenized_tail = spacy_tokenize(tail)\n",
    "    \n",
    "    head_indices = None\n",
    "    tail_indices = None\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_head[0] and tokens[i:i+len(tokenized_head)] == tokenized_head:\n",
    "            head_indices = list(range(i,i+len(tokenized_head)))\n",
    "            break\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_tail[0] and tokens[i:i+len(tokenized_tail)] == tokenized_tail:\n",
    "            tail_indices = list(range(i,i+len(tokenized_tail)))\n",
    "            break\n",
    "    if head_indices is None or tail_indices is None:\n",
    "        print(sentence)\n",
    "        print(head)\n",
    "        print(tail)\n",
    "        raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")\n",
    "df['reldescription'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = dfi['reldescription'].value_counts()\n",
    "cnt = cnt[cnt >= 3]\n",
    "dfi = dfi[dfi['reldescription'].isin(cnt.index)].copy().reset_index(drop=True)\n",
    "dft = dfi[dfi['reldescription'] == 'media platform']\n",
    "dft.sample(3, replace=True, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_framework import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader.load_relation_support_csv(\"connections_Mueller_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint 'checkpoint/pair-bert-train_wiki-val_wiki-5-3.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "from fyp_detection_framework import DetectionFramework\n",
    "d = DetectionFramework(ckpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3.pth.tar\")\n",
    "# d.load_support(\"connections_Mueller_cleaned.csv\", min_instance=9)\n",
    "d.load_support(\"test_relation_support_dataset.csv\")\n",
    "d.load_queries(\"test_queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['funding', 'meeting', 'hacking', 'oppose', 'assistance']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(list(r['name'] for r in d.support))\n",
    "list(r['name'] for r in d.support)\n",
    "# d.support = [i for i in d.support if i['name'] != 'oppose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Trump hacked Obama in 2017.\", head: \"Trump\", tail: \"Obama\", prediction: meeting\n",
      "Sentence: \"Trump hacked Obama in 2017.\", head: \"Obama\", tail: \"Trump\", prediction: hacking\n",
      "Sentence: \"Trump had had contact, including a meeting in 2010, with Obama before he became President. \", head: \"Trump\", tail: \"Obama\", prediction: meeting\n",
      "Sentence: \"Trump had had contact, including a meeting in 2010, with Obama before he became President. \", head: \"Obama\", tail: \"Trump\", prediction: meeting\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Trump\", tail: \"Obama\", prediction: oppose\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Obama\", tail: \"Trump\", prediction: hacking\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Congress\", tail: \"Obama\", prediction: assistance\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Obama\", tail: \"Congress\", prediction: hacking\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Congress\", tail: \"Trump\", prediction: assistance\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Trump\", tail: \"Congress\", prediction: assistance\n",
      "Sentence: \"Trump had opposed President Obama's invasion of Iraq.\", head: \"Trump\", tail: \"Obama\", prediction: oppose\n",
      "Sentence: \"Trump had opposed President Obama's invasion of Iraq.\", head: \"Obama\", tail: \"Trump\", prediction: meeting\n",
      "Sentence: \"Trump had opposed President Obama's invasion of Iraq.\", head: \"Iraq\", tail: \"Obama\", prediction: oppose\n",
      "Sentence: \"Trump had opposed President Obama's invasion of Iraq.\", head: \"Obama\", tail: \"Iraq\", prediction: funding\n",
      "Sentence: \"Trump had opposed President Obama's invasion of Iraq.\", head: \"Iraq\", tail: \"Trump\", prediction: hacking\n",
      "Sentence: \"Trump had opposed President Obama's invasion of Iraq.\", head: \"Trump\", tail: \"Iraq\", prediction: oppose\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Trump hacked Obama in 2017.',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'meeting',\n",
       "  tensor([[[ 1.1687,  2.9192,  0.7229,  2.1924,  1.0462, -5.6299]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump hacked Obama in 2017.',\n",
       "  'Obama',\n",
       "  'Trump',\n",
       "  'hacking',\n",
       "  tensor([[[ 0.7097,  2.6125,  5.6731,  2.9651,  3.7836, -5.5936]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had had contact, including a meeting in 2010, with Obama before he became President. ',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'meeting',\n",
       "  tensor([[[ 2.5912,  4.2093,  0.8967,  3.8662,  2.5983, -5.4393]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had had contact, including a meeting in 2010, with Obama before he became President. ',\n",
       "  'Obama',\n",
       "  'Trump',\n",
       "  'meeting',\n",
       "  tensor([[[ 2.2640,  4.3324,  1.6256,  2.3130,  2.5719, -5.1768]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'oppose',\n",
       "  tensor([[[ 3.9116,  3.4033,  0.9705,  3.9855,  2.8832, -5.0100]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Obama',\n",
       "  'Trump',\n",
       "  'hacking',\n",
       "  tensor([[[ 2.1624,  3.7131,  4.9463,  2.7108,  4.3063, -4.5245]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Congress',\n",
       "  'Obama',\n",
       "  'assistance',\n",
       "  tensor([[[ 2.6420,  3.4160,  6.1199,  4.8037,  7.3588, -3.6859]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Obama',\n",
       "  'Congress',\n",
       "  'hacking',\n",
       "  tensor([[[ 0.3377,  3.0158,  5.2172,  0.0489,  3.6856, -6.1680]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Congress',\n",
       "  'Trump',\n",
       "  'assistance',\n",
       "  tensor([[[ 4.3338,  4.2516,  7.4561,  6.1338,  8.7894, -2.0873]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " ('Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Trump',\n",
       "  'Congress',\n",
       "  'assistance',\n",
       "  tensor([[[ 4.0891,  3.5819,  4.6913,  1.5365,  5.0952, -5.2737]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump had opposed President Obama's invasion of Iraq.\",\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'oppose',\n",
       "  tensor([[[ 8.0370e-01,  2.5445e+00,  6.1695e-04,  5.3715e+00,  2.3004e+00,\n",
       "            -5.8660e+00]]], grad_fn=<CatBackward>)),\n",
       " (\"Trump had opposed President Obama's invasion of Iraq.\",\n",
       "  'Obama',\n",
       "  'Trump',\n",
       "  'meeting',\n",
       "  tensor([[[ 2.1074,  2.5157, -1.0623,  1.0673, -0.5976, -6.5027]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump had opposed President Obama's invasion of Iraq.\",\n",
       "  'Iraq',\n",
       "  'Obama',\n",
       "  'oppose',\n",
       "  tensor([[[-3.4609, -2.0250,  2.1009,  2.1704, -0.2658, -6.8858]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump had opposed President Obama's invasion of Iraq.\",\n",
       "  'Obama',\n",
       "  'Iraq',\n",
       "  'funding',\n",
       "  tensor([[[-0.5042, -1.6212, -4.3934, -2.3105, -4.3712, -6.9158]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump had opposed President Obama's invasion of Iraq.\",\n",
       "  'Iraq',\n",
       "  'Trump',\n",
       "  'hacking',\n",
       "  tensor([[[-3.9349, -1.9881,  2.6522,  1.0460,  0.3397, -6.8770]]],\n",
       "         grad_fn=<CatBackward>)),\n",
       " (\"Trump had opposed President Obama's invasion of Iraq.\",\n",
       "  'Trump',\n",
       "  'Iraq',\n",
       "  'oppose',\n",
       "  tensor([[[-0.1058, -0.4361, -2.0515,  0.3583, -0.8021, -6.6858]]],\n",
       "         grad_fn=<CatBackward>))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
