{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FewRel few-shot relation extraction\n",
    "\n",
    "* refer to fewrel_framework_project_readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The current code is at the very bottom. the rest of the code is just for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/pair-bert-train_wiki-val_wiki-5-1.pth.tar\"\n",
    "bert_pretrained_checkpoint = 'bert-base-uncased'\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewshot_re_kit.data_loader import FewRelDatasetPair, get_loader_pair\n",
    "from fewshot_re_kit.framework import FewShotREFramework\n",
    "from fewshot_re_kit.sentence_encoder import BERTPAIRSentenceEncoder\n",
    "from models.pair import Pair\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import neuralcoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = BERTPAIRSentenceEncoder(\n",
    "                    bert_pretrained_checkpoint,\n",
    "                    max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meow_loader = get_loader_pair('val_wiki', sentence_encoder,\n",
    "#                 N=5, K=1, Q=1, na_rate=0, batch_size=1, encoder_name='bert')\n",
    "\n",
    "val_data_loader = iter(FewRelDatasetPair('val_wiki', sentence_encoder, N=5, K=1, Q=1, na_rate=0, root='./data', encoder_name='bert'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pair(sentence_encoder, hidden_size=768)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(next(val_data_loader)[0]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load_model__(ckpt):\n",
    "    '''\n",
    "    ckpt: Path of the checkpoint\n",
    "    return: Checkpoint dict\n",
    "    '''\n",
    "    if os.path.isfile(ckpt):\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        print(\"Successfully loaded checkpoint '%s'\" % ckpt)\n",
    "        return checkpoint\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found at '%s'\" % ckpt)\n",
    "\n",
    "        \n",
    "def item(x):\n",
    "    '''\n",
    "    PyTorch before and after 0.4\n",
    "    '''\n",
    "    torch_version = torch.__version__.split('.')\n",
    "    if int(torch_version[0]) == 0 and int(torch_version[1]) < 4:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x.item()\n",
    "    \n",
    "def bert_tokenize(tokens, head_indices, tail_indices):\n",
    "    word = sentence_encoder.tokenize(tokens,\n",
    "            head_indices,\n",
    "            tail_indices)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "list(map(str, tokenizer(\"\"\"hello meow. meow is donald trump's friend\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from the model checkpoint state\n",
    "\n",
    "model.eval()\n",
    "state_dict = __load_model__(checkpoint_path)['state_dict']\n",
    "own_state = model.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    if name not in own_state:\n",
    "        continue\n",
    "    own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on the wikidata dataset, which is what they have already implemented.\n",
    "\n",
    "N = 5\n",
    "K = 1\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "with torch.no_grad():\n",
    "    for it in range(10):\n",
    "        batch, label = next(val_data_loader)\n",
    "        label = torch.tensor(label)\n",
    "        batch['word'] = torch.stack(batch['word'])\n",
    "        batch['seg'] = torch.stack(batch['seg'])\n",
    "        batch['mask'] = torch.stack(batch['mask'])\n",
    "        logits, pred = model(batch, N, K, Q * N + Q * na_rate)\n",
    "        print(pred, label)\n",
    "        right = model.accuracy(pred, label)\n",
    "        print(item(right.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "K = 2\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "#names have to be upper case, otherwise they are not detected by NER\n",
    "example_relation_data = [\n",
    "    {'name':'love',\n",
    "    'examples':[\n",
    "        {'sentence':'Meow loves Mo', 'head':'Meow', 'tail':'Mo'},\n",
    "        {'sentence':'Tom is in love with Jull', 'head':'Tom', 'tail':'Jull'}\n",
    "    ]},\n",
    "    {'name':'hate',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump hates the Mooch', 'head':'Trump', 'tail':'Mooch'},\n",
    "        {'sentence':'Ivanka and Jared dislike each other intensely', 'head':'Ivanka', 'tail':'Jared'}\n",
    "    ]},\n",
    "    {'name':'spouse',\n",
    "    'examples':[\n",
    "        {'sentence':'Trump is married to Ivanka', 'head':'Trump', 'tail':'Ivanka'},\n",
    "        {'sentence':\"Bill went out with his wife Jill on saturday\", 'head':'Bill', 'tail':'Jill'}\n",
    "    ]},\n",
    "        {'name':'insult',\n",
    "    'examples':[\n",
    "        {'sentence':'The President said that Michael Cohen is a rat', 'head':'The President', 'tail':'Michael'},\n",
    "        {'sentence':'Meow and Tom threw jabs at each other', 'head':'Meow', 'tail':'Tom'}\n",
    "    ]},\n",
    "        {'name':'capital',\n",
    "    'examples':[\n",
    "        {'sentence':'Austin is the capital of Texas', 'head':'Austin', 'tail':'Texas'},\n",
    "        {'sentence':\"the capital of China is located in Beijing\", 'head':'Beijing', 'tail':\"China\"}\n",
    "    ]}\n",
    "    \n",
    "]\n",
    "\n",
    "queries = [{\n",
    "    'sentence':'Cohen and Fluffy are very loving to each other','head':'Cohen','tail':'Fluffy'\n",
    "},\n",
    "{\n",
    "    'sentence':\"\"\"The US's capital is Washington\"\"\",'head':'Washington','tail':'US'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using already specified entities in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    return list(map(str, nlp(sentence)))\n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "#     tokens = q['sentence'].split(\" \")  #TODO: generalize, make it tokenize like in the example wikidata, would probably need to use some nlp library to do it\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    tokenized_head = spacy_tokenize(q['head'])\n",
    "    tokenized_tail = spacy_tokenize(q['tail'])\n",
    "    head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   #TODO: make it work with multi-word entities\n",
    "    tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "    bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "    for relation in example_relation_data:\n",
    "        for ex in relation['examples']:\n",
    "#             tokens = ex['sentence'].split(\" \")  #TODO: generalize\n",
    "            tokens = spacy_tokenize(ex['sentence'])\n",
    "            tokenized_head = spacy_tokenize(ex['head'])\n",
    "            tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "            head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "            tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "            bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "            \n",
    "            SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "            word_tensor = torch.zeros((max_length)).long()\n",
    "            \n",
    "            new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "            for i in range(min(max_length, len(new_word))):\n",
    "                word_tensor[i] = new_word[i]\n",
    "            mask_tensor = torch.zeros((max_length)).long()\n",
    "            mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "            seg_tensor = torch.ones((max_length)).long()\n",
    "            seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "            fusion_set['word'].append(word_tensor)\n",
    "            fusion_set['mask'].append(mask_tensor)\n",
    "            fusion_set['seg'].append(seg_tensor)\n",
    "    \n",
    "    fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "    fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "    fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "    logits, pred = model(fusion_set, N, K, 1)\n",
    "    print(pred, logits)\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# ex = \"hello Meow. Meow is Donald Trump's friend\"\n",
    "ex = \"\"\"The US's capital is Washington\"\"\"\n",
    "doc = nlp(ex)\n",
    "print(doc._.coref_resolved)\n",
    "doc = nlp(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(str, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using entities detected in the query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, nlp(doc._.coref_resolved)))\n",
    "\n",
    "def get_head_tail_pairs(sentence):\n",
    "    acceptable_entity_types = ['PERSON', 'NORP', 'ORG', 'GPE', 'PRODUCT', 'EVENT', 'LAW', 'LOC', 'FAC']\n",
    "    doc = nlp(sentence)\n",
    "    doc = nlp(doc._.coref_resolved)\n",
    "    entity_info = [(X.text, X.label_) for X in doc.ents]\n",
    "    entity_info = set(map(lambda x:x[0], filter(lambda x:x[1] in acceptable_entity_types, entity_info)))\n",
    "\n",
    "    return combinations(entity_info, 2)\n",
    "    \n",
    "\n",
    "max_length = 128   #max length in terms of the number of characters - by default it was 128, seems to work with longer lengths also though.\n",
    "# the actual length of the sentence doesn't matter, only the number of bert tokens which are created. and this is managed automatically. \n",
    "\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    \n",
    "    \n",
    "    for head, tail in get_head_tail_pairs(q['sentence']):  #iterating through all possible combinations of 2 named entities\n",
    "        tokenized_head = spacy_tokenize(head)\n",
    "        tokenized_tail = spacy_tokenize(tail)\n",
    "        head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   \n",
    "        tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "        bert_query_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "        for relation in example_relation_data:\n",
    "            for ex in relation['examples']:\n",
    "                tokens = spacy_tokenize(ex['sentence'])\n",
    "                tokenized_head = spacy_tokenize(ex['head'])  #head and tail spelling and punctuation should match the corefered output exactly\n",
    "                tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "                head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "                tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "                bert_relation_example_tokens = bert_tokenize(tokens, head_indices, tail_indices)\n",
    "\n",
    "                SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "                CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "                word_tensor = torch.zeros((max_length)).long()\n",
    "\n",
    "                new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "                for i in range(min(max_length, len(new_word))):\n",
    "                    word_tensor[i] = new_word[i]\n",
    "                mask_tensor = torch.zeros((max_length)).long()\n",
    "                mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "                seg_tensor = torch.ones((max_length)).long()\n",
    "                seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "                fusion_set['word'].append(word_tensor)\n",
    "                fusion_set['mask'].append(mask_tensor)\n",
    "                fusion_set['seg'].append(seg_tensor)\n",
    "\n",
    "        fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "        fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "        fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            fusion_set['word'] = fusion_set['word'].cuda()\n",
    "            fusion_set['seg'] = fusion_set['seg'].cuda()\n",
    "            fusion_set['mask'] = fusion_set['mask'].cuda()\n",
    "        \n",
    "        logits, pred = model(fusion_set, N, K, 1)\n",
    "        print('Sentence: \\\"{}\\\", head: \\\"{}\\\", tail: \\\"{}\\\", prediction: {}'.format(q['sentence'], head, tail, example_relation_data[pred.item()]['name']))   #TODO: handle na case, which would be out of bounds\n",
    "    \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_functions import Detector\n",
    "d = Detector(chpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3.pth.tar\")\n",
    "d.run_on_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Mueller connections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code can be used to go through the dataset and make sure that the heads and tails are accurate.\n",
    "\n",
    "#use utf 8 encoding if there are errors. can open the csv in sublime and resave it to use utf 8 encoding.\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")   #this csv file is heavily modified from the original, it has been cleaned to make sure that the heads and tails actually exist.\n",
    "#it could be cleaned even further by getting rid of useless relations etc.\n",
    "dfi = df[['sentence', 'head', 'tail', 'reldescription']].copy()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")    #no coref being done here, the assumption is that no coref will be done on the support/training data, only on test data.\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return list(map(str, doc))\n",
    "\n",
    "for ind, row in dfi.iterrows():\n",
    "#     if ind < 304:\n",
    "#         continue\n",
    "    head = row['head']\n",
    "    tail = row['tail']\n",
    "    sentence = row['sentence']\n",
    "    \n",
    "    tokens = spacy_tokenize(sentence)\n",
    "    \n",
    "#     print(ind)\n",
    "    tokenized_head = spacy_tokenize(head)\n",
    "    tokenized_tail = spacy_tokenize(tail)\n",
    "    \n",
    "    head_indices = None\n",
    "    tail_indices = None\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_head[0] and tokens[i:i+len(tokenized_head)] == tokenized_head:\n",
    "            head_indices = list(range(i,i+len(tokenized_head)))\n",
    "            break\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == tokenized_tail[0] and tokens[i:i+len(tokenized_tail)] == tokenized_tail:\n",
    "            tail_indices = list(range(i,i+len(tokenized_tail)))\n",
    "            break\n",
    "    if head_indices is None or tail_indices is None:\n",
    "        print(sentence)\n",
    "        print(head)\n",
    "        print(tail)\n",
    "        raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"connections_Mueller_cleaned.csv\")\n",
    "df['reldescription'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = dfi['reldescription'].value_counts()\n",
    "cnt = cnt[cnt >= 3]\n",
    "dfi = dfi[dfi['reldescription'].isin(cnt.index)].copy().reset_index(drop=True)\n",
    "dft = dfi[dfi['reldescription'] == 'media platform']\n",
    "dft.sample(3, replace=True, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp_detection_framework import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader.load_relation_support_csv(\"connections_Mueller_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing the current version - from below only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint 'checkpoint/pair-bert-train_wiki-val_wiki-5-3-na3.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "from fyp_detection_framework import DetectionFramework\n",
    "d = DetectionFramework(ckpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3-na3.pth.tar\")\n",
    "# d = DetectionFramework(ckpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-6-4-na3.pth.tar\")\n",
    "# d = DetectionFramework(ckpt_path=\"checkpoint/pair-bert-train_wiki-val_wiki-5-3.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.clear_support_queries()\n",
    "# d.load_support(\"connections_Mueller_cleaned.csv\", min_instance=9)\n",
    "# d.load_support(\"test_relation_support_dataset.csv\", K=5)\n",
    "d.load_support(\"test_relation_support_dataset_2.csv\", K=5)\n",
    "# d.load_support(\"test_relation_support_dataset_3.csv\", K=5)\n",
    "# d.load_queries_csv(\"test_queries.csv\")\n",
    "d.load_queries_predefined_head_tail_csv(\"test_queries_with_head_tail_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coordination', 'contact', 'assistance', 'oppose', 'part_of']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(list(r['name'] for r in d.support))\n",
    "list(r['name'] for r in d.support)\n",
    "# d.support = [i for i in d.support if i['name'] != 'oppose']\n",
    "# d.support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The dean of HKUST interviewed Carrie Lam, but it was inconclusive.\", head: \"dean of HKUST\", tail: \"Carrie Lam\", prediction: assistance\n",
      "Sentence: \"Trump hacked Obama's laptop in 2017.\", head: \"Obama's laptop\", tail: \"Trump\", prediction: oppose\n",
      "Sentence: \"Trump had had contact, including a meeting in 2010, with Obama before he became President. \", head: \"Trump\", tail: \"Obama\", prediction: contact\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Trump\", tail: \"Obama\", prediction: contact\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Trump\", tail: \"Congress\", prediction: part_of\n",
      "Sentence: \"Trump opposed President Obama's invasion of Iraq.\", head: \"invasion of Iraq\", tail: \"Trump\", prediction: oppose\n",
      "Sentence: \"Trump had gotten funding from Congress to investigate Obama.\", head: \"Congress\", tail: \"Trump\", prediction: assistance\n",
      "Sentence: \"Trump objected to President Obama's invasion of Iraq.\", head: \"invasion of Iraq\", tail: \"Trump\", prediction: oppose\n",
      "Sentence: \"The professor had asked for funding from the college, but it was too little too late.\", head: \"The professor\", tail: \"the college\", prediction: assistance\n",
      "Sentence: \"The US CDC offered to help the Chinese government in solving the virus problem.\", head: \"US CDC\", tail: \"Chinese government\", prediction: assistance\n",
      "Sentence: \"Many fake accounts had been identified by the NSA in the past, but there were only deleted in the last 2 weeks. \", head: \"NSA\", tail: \"fake accounts\", prediction: oppose\n",
      "Sentence: \"Ms. Warren is seeking do better than her third-place standing in Iowa, helping create momentum for later states and supplant Mr. Buttigieg as the candidate pitching \"unity\" to a frightened Democratic electorate.\", head: \"Ms. Warren\", tail: \"Mr. Buttigieg\", prediction: NA\n",
      "Sentence: \"Mr. Hammill shot back at Mr. Stone, writing: \"What planet are you living on? this is deceptively altered. take it down.\"\", head: \"Mr. Hammill\", tail: \"Mr. Stone\", prediction: part_of\n",
      "Sentence: \"But Rabih Shaer, founder of a Lebanese nonprofit that campaigns against corruption, called the government's sluggish response \"irresponsible and criminal\"\", head: \"Rabih Shaer\", tail: \"government\", prediction: part_of\n",
      "Sentence: \"Because the moderate wing opposing Mr. Sanders, a Vermont liberal, is so fragmented, the lower-than-hoped-for turnout has not slowed his ascent.\", head: \"moderate wing\", tail: \"Mr. Sanders\", prediction: oppose\n",
      "Sentence: \"Mr. Trump said that he and Mr. Modi would eventually be making “very, very major” trade deals, but added that they are in the \"early stages of discussion.\"\", head: \"Mr. Trump\", tail: \"Mr. Modi\", prediction: contact\n",
      "Sentence: \"When he became prime minister in the spring of 2014, the travel ban was lifted, and later that year, Mr. Modi made his first triumphant visit to the United States, where he had a private dinner with President Obama.\", head: \"Mr. Modi\", tail: \"President Obama\", prediction: NA\n",
      "Sentence: \"But the George W. Bush administration was suspicious enough of Mr. Modi's role to ban him in 2005 from visiting the United States.\", head: \"George W. Bush administration\", tail: \"Mr. Modi\", prediction: assistance\n",
      "Sentence: \"Perhaps because of the channel's popularity with Democrats, Mr. Sanders’s campaign has singled out MSNBC for criticism, complaining about Mr. Matthews and the political anchor Chuck Todd, who recently read on his program a column by a conservative writer that referred to Mr. Sanders’s aggressive online supporters as “brownshirts.”\", head: \"Mr. Sanders’s campaign\", tail: \"Mr. Matthews\", prediction: assistance\n",
      "Sentence: \"Mr. Sanders, for his part, moved quickly to denounce Russia, calling President Vladimir V. Putin an \"autocratic thug\" and warning Moscow to stay out of the election.\", head: \"Mr. Sanders\", tail: \"President Vladimir V. Putin\", prediction: NA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The dean of HKUST interviewed Carrie Lam, but it was inconclusive.',\n",
       "  'dean of HKUST',\n",
       "  'Carrie Lam',\n",
       "  'assistance',\n",
       "  tensor([[[0.6553, 0.3769, 2.7187, 1.9786, 1.4172, 0.4444]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " [\"Trump hacked Obama's laptop in 2017.\",\n",
       "  \"Obama's laptop\",\n",
       "  'Trump',\n",
       "  'oppose',\n",
       "  tensor([[[ 0.7937,  1.4791,  1.8642,  2.7131, -0.6661,  0.4212]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: -0.4019, neg: 0.351, neu: 0.649, pos: 0.0, '],\n",
       " ['Trump had had contact, including a meeting in 2010, with Obama before he became President. ',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'contact',\n",
       "  tensor([[[-0.1533,  1.7125,  0.5415,  0.1885,  0.6207,  0.5909]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Trump',\n",
       "  'Obama',\n",
       "  'contact',\n",
       "  tensor([[[0.3326, 1.8534, 1.5320, 1.1228, 0.9927, 0.5690]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Trump',\n",
       "  'Congress',\n",
       "  'part_of',\n",
       "  tensor([[[ 0.0169,  0.7425,  1.7721, -0.0563,  2.2726,  0.5755]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " [\"Trump opposed President Obama's invasion of Iraq.\",\n",
       "  'invasion of Iraq',\n",
       "  'Trump',\n",
       "  'oppose',\n",
       "  tensor([[[-0.8634, -0.5912,  0.3703,  1.0249, -3.5445,  0.8891]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['Trump had gotten funding from Congress to investigate Obama.',\n",
       "  'Congress',\n",
       "  'Trump',\n",
       "  'assistance',\n",
       "  tensor([[[ 1.3854,  1.6256,  4.7709,  2.2520,  0.2157, -0.0958]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " [\"Trump objected to President Obama's invasion of Iraq.\",\n",
       "  'invasion of Iraq',\n",
       "  'Trump',\n",
       "  'oppose',\n",
       "  tensor([[[-0.3834, -0.1237,  0.6727,  1.0724, -3.1057,  0.8462]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['The professor had asked for funding from the college, but it was too little too late.',\n",
       "  'The professor',\n",
       "  'the college',\n",
       "  'assistance',\n",
       "  tensor([[[0.5895, 0.8045, 2.2495, 0.8259, 2.1516, 0.4347]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['The US CDC offered to help the Chinese government in solving the virus problem.',\n",
       "  'US CDC',\n",
       "  'Chinese government',\n",
       "  'assistance',\n",
       "  tensor([[[ 1.6364,  0.1123,  4.0849,  0.6348, -0.4124,  0.0966]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.34, neg: 0.144, neu: 0.585, pos: 0.271, '],\n",
       " ['Many fake accounts had been identified by the NSA in the past, but there were only deleted in the last 2 weeks. ',\n",
       "  'NSA',\n",
       "  'fake accounts',\n",
       "  'oppose',\n",
       "  tensor([[[0.7712, 0.7756, 1.6008, 2.5031, 0.3641, 0.4851]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: -0.2617, neg: 0.093, neu: 0.907, pos: 0.0, '],\n",
       " ['Ms. Warren is seeking do better than her third-place standing in Iowa, helping create momentum for later states and supplant Mr. Buttigieg as the candidate pitching \"unity\" to a frightened Democratic electorate.',\n",
       "  'Ms. Warren',\n",
       "  'Mr. Buttigieg',\n",
       "  'NA',\n",
       "  tensor([[[-0.7862, -0.9144, -1.1716, -1.4482,  0.1508,  0.7877]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.5106, neg: 0.078, neu: 0.728, pos: 0.194, '],\n",
       " ['Mr. Hammill shot back at Mr. Stone, writing: \"What planet are you living on? this is deceptively altered. take it down.\"',\n",
       "  'Mr. Hammill',\n",
       "  'Mr. Stone',\n",
       "  'part_of',\n",
       "  tensor([[[-0.1839,  0.1282, -0.2458, -0.2459,  1.3093,  0.8081]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['But Rabih Shaer, founder of a Lebanese nonprofit that campaigns against corruption, called the government\\'s sluggish response \"irresponsible and criminal\"',\n",
       "  'Rabih Shaer',\n",
       "  'government',\n",
       "  'part_of',\n",
       "  tensor([[[-1.8231, -0.7851, -0.8767, -1.7819,  0.8783,  0.6781]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: -0.8402, neg: 0.36, neu: 0.64, pos: 0.0, '],\n",
       " ['Because the moderate wing opposing Mr. Sanders, a Vermont liberal, is so fragmented, the lower-than-hoped-for turnout has not slowed his ascent.',\n",
       "  'moderate wing',\n",
       "  'Mr. Sanders',\n",
       "  'oppose',\n",
       "  tensor([[[-0.7157, -1.3929,  1.3594,  1.5236, -0.4516,  0.5523]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['Mr. Trump said that he and Mr. Modi would eventually be making “very, very major” trade deals, but added that they are in the \"early stages of discussion.\"',\n",
       "  'Mr. Trump',\n",
       "  'Mr. Modi',\n",
       "  'contact',\n",
       "  tensor([[[2.2612, 2.5854, 2.4606, 2.1357, 2.1899, 0.3780]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, '],\n",
       " ['When he became prime minister in the spring of 2014, the travel ban was lifted, and later that year, Mr. Modi made his first triumphant visit to the United States, where he had a private dinner with President Obama.',\n",
       "  'Mr. Modi',\n",
       "  'President Obama',\n",
       "  'NA',\n",
       "  tensor([[[-0.9699, -0.1858, -1.1754, -1.2540, -0.7513,  0.9598]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.3818, neg: 0.08, neu: 0.781, pos: 0.138, '],\n",
       " [\"But the George W. Bush administration was suspicious enough of Mr. Modi's role to ban him in 2005 from visiting the United States.\",\n",
       "  'George W. Bush administration',\n",
       "  'Mr. Modi',\n",
       "  'assistance',\n",
       "  tensor([[[ 1.5255,  1.7187,  4.4427,  1.8020,  1.4764, -0.0857]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: -0.5106, neg: 0.211, neu: 0.692, pos: 0.097, '],\n",
       " [\"Perhaps because of the channel's popularity with Democrats, Mr. Sanders’s campaign has singled out MSNBC for criticism, complaining about Mr. Matthews and the political anchor Chuck Todd, who recently read on his program a column by a conservative writer that referred to Mr. Sanders’s aggressive online supporters as “brownshirts.”\",\n",
       "  'Mr. Sanders’s campaign',\n",
       "  'Mr. Matthews',\n",
       "  'assistance',\n",
       "  tensor([[[0.4459, 0.5559, 2.5317, 1.9394, 0.7884, 0.4308]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: 0.1779, neg: 0.116, neu: 0.773, pos: 0.11, '],\n",
       " ['Mr. Sanders, for his part,\\xa0moved quickly to denounce Russia, calling President Vladimir V. Putin an \"autocratic thug\" and warning Moscow to stay out of the election.',\n",
       "  'Mr. Sanders',\n",
       "  'President Vladimir V. Putin',\n",
       "  'NA',\n",
       "  tensor([[[-0.9351,  0.4548, -0.4075, -1.2736,  0.3666,  0.9150]]],\n",
       "         grad_fn=<CatBackward>),\n",
       "  'compound: -0.5859, neg: 0.161, neu: 0.839, pos: 0.0, ']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.support[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
