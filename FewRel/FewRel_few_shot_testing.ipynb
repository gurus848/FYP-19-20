{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FewRel few-shot relation extraction\n",
    "\n",
    "* goal is to make it like the demo at http://opennre.thunlp.ai/#/fewshot_re - in the demo it seems to work pretty well\n",
    "* based on https://github.com/thunlp/FewRel\n",
    "* use torch 1.3.1\n",
    "* copy val_wiki.json as test_wiki.json in the data folder to make it work\n",
    "* python requirements same as opennre\n",
    "* the checkpoint files are very big, so they aren't in the repository. One checkpoint is at https://drive.google.com/file/d/1yiz3q3xNz-llsY55g5OdodxiH1RThYuz/view?usp=sharing\n",
    "* can't train on prof song's gpus, not enough vram. the hpc computers have enough vram, but i couldn't get pytorch to work on them, it uses a 10 year old version of linux.... \n",
    "* however testing using this code does actually work on the gpu (but you would need to use cuda pytorch (refer to pytorch.org) and whereever you are using model do \"model = model.cuda()\", and also \"tensor=tensor.cuda()\"). refer to test_script.py\n",
    "* on my laptop cpu this code takes about 2 seconds per query, on the gpu it's a lot faster, runs in under 1 second. the speed seems fine, but more testing is required to really find out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/pair-bert-train_wiki-val_wiki-5-1.pth.tar\"\n",
    "bert_pretrained_checkpoint = 'bert-base-uncased'\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewshot_re_kit.data_loader import FewRelDatasetPair, get_loader_pair\n",
    "from fewshot_re_kit.framework import FewShotREFramework\n",
    "from fewshot_re_kit.sentence_encoder import BERTPAIRSentenceEncoder\n",
    "from models.pair import Pair\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import neuralcoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0112 21:57:26.851216 140734809875904 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/GuruSenthil/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I0112 21:57:26.852715 140734809875904 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0112 21:57:28.557618 140734809875904 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/GuruSenthil/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0112 21:57:31.475033 140734809875904 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0112 21:57:31.475764 140734809875904 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "I0112 21:57:32.646106 140734809875904 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/GuruSenthil/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder = BERTPAIRSentenceEncoder(\n",
    "                    bert_pretrained_checkpoint,\n",
    "                    max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meow_loader = get_loader_pair('val_wiki', sentence_encoder,\n",
    "#                 N=5, K=1, Q=1, na_rate=0, batch_size=1, encoder_name='bert')\n",
    "\n",
    "val_data_loader = iter(FewRelDatasetPair('val_wiki', sentence_encoder, N=5, K=1, Q=1, na_rate=0, root='./data', encoder_name='bert'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pair(sentence_encoder, hidden_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Since', '1962', 'the', 'claimant', 'to', 'the', 'throne', 'has', 'been', 'Taw', 'Phaya', ',', 'the', 'second', 'son', 'of', 'Princess', 'Myat', 'Phaya', '.'], 'h': ['taw phaya', 'Q3601421', [[9, 10]]], 't': ['myat phaya', 'Q6946802', [[17, 18]]]}\n",
      "{'tokens': ['His', 'wife', 'Nonia', 'Celsa', 'bore', 'him', 'a', 'son', ',', 'Diadumenianus', ',', 'whom', 'he', 'made', 'co', '-', 'Emperor', 'in', '218', ';', 'both', 'were', 'executed', 'by', 'partisans', 'of', '\"', 'Elagabalus', '\"', '(', 'see', 'below', ')', '.'], 'h': ['diadumenianus', 'Q46840', [[9]]], 't': ['nonia celsa', 'Q2724125', [[2, 3]]]}\n",
      "{'tokens': ['Major', 'General', 'Robert', 'Maxwell', 'Johnstone', '(', '9', 'March', '1914', '–', '11', 'March', '1990', ')', 'was', 'a', 'senior', 'British', 'Army', 'officer', '.'], 'h': ['robert maxwell johnstone', 'Q23071389', [[2, 3, 4]]], 't': ['major general', 'Q287709', [[0, 1]]]}\n",
      "{'tokens': ['At', 'the', 'same', 'time', 'the', '11th', '(', 'East', 'Africa', ')', 'Division', 'under', 'Major', 'General', 'Charles', 'Fowkes', '(', 'also', 'under', 'XXXIII', 'Corps', ')', 'cleared', 'the', 'Kabaw', 'Valley', ',', 'later', 'establishing', 'a', 'bridgehead', 'across', 'the', 'Chindwin', 'River', '.'], 'h': ['charles fowkes', 'Q1621938', [[14, 15]]], 't': ['major general', 'Q287709', [[12, 13]]]}\n",
      "{'tokens': ['The', 'Myanmar', 'Air', 'Force', '(', ',', ')', ',', 'known', 'until', '1989', 'as', 'the', 'Burmese', 'Air', 'Force', ',', 'is', 'the', 'aerial', 'branch', 'of', 'Myanmar', \"'s\", 'armed', 'forces', ',', 'the', 'Tatmadaw', '.'], 'h': ['myanmar air force', 'Q4920908', [[1, 2, 3]]], 't': ['tatmadaw', 'Q728190', [[28]]]}\n",
      "{'tokens': ['Crowlands', 'was', 'an', 'electoral', 'district', 'of', 'the', 'Legislative', 'Assembly', '\\n ', 'in', 'the', 'Australian', 'colony', 'of', 'Victoria', 'from', '1859', 'to', '1877', '.'], 'h': ['electoral district', 'Q5356187', [[3, 4]]], 't': ['legislative assembly', 'Q4386693', [[7, 8]]]}\n",
      "{'tokens': ['David', 'Kircus', '(', 'born', 'February', '19', ',', '1980', ')', 'is', 'a', 'former', 'gridiron', 'football', 'wide', 'receiver', '.'], 'h': ['david kircus', 'Q14951106', [[0, 1]]], 't': ['wide receiver', 'Q918224', [[14, 15]]]}\n",
      "{'tokens': ['On', 'the', 'Hokies', \"'\", 'second', 'offensive', 'possession', ',', 'quarterback', 'Tyrod', 'Taylor', 'took', 'the', 'field', 'in', 'place', 'of', 'Sean', 'Glennon', '.'], 'h': ['tyrod taylor', 'Q2591444', [[9, 10]]], 't': ['quarterback', 'Q622747', [[8]]]}\n",
      "{'tokens': ['Kekuiapoiwa', 'Liliha', 'married', 'Kīwalaʻō', 'and', 'their', 'child', 'was', 'Queen', 'Keōpūolani', ',', 'consort', 'of', 'Kamehameha', 'I', 'and', 'mother', 'of', 'two', 'kings', '.'], 'h': ['kīwalaʻō', 'Q6454637', [[3]]], 't': ['keōpūolani', 'Q3439080', [[9]]]}\n",
      "{'tokens': ['Wollstonecraft', 'named', 'her', 'daughter', ',', 'Fanny', 'Imlay', '(', '1794', '-', '1816', ')', ',', 'after', 'her', 'friend', '.'], 'h': ['wollstonecraft', 'Q101638', [[0]]], 't': ['fanny imlay', 'Q437872', [[5, 6]]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(val_data_loader)[0]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load_model__(ckpt):\n",
    "    '''\n",
    "    ckpt: Path of the checkpoint\n",
    "    return: Checkpoint dict\n",
    "    '''\n",
    "    if os.path.isfile(ckpt):\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        print(\"Successfully loaded checkpoint '%s'\" % ckpt)\n",
    "        return checkpoint\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found at '%s'\" % ckpt)\n",
    "\n",
    "        \n",
    "def item(x):\n",
    "    '''\n",
    "    PyTorch before and after 0.4\n",
    "    '''\n",
    "    torch_version = torch.__version__.split('.')\n",
    "    if int(torch_version[0]) == 0 and int(torch_version[1]) < 4:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x.item()\n",
    "    \n",
    "def tokenize(tokens, head_indices, tail_indices):\n",
    "    word = sentence_encoder.tokenize(tokens,\n",
    "            head_indices,\n",
    "            tail_indices)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'meow', '.', 'meow', 'is', 'donald', 'trump', \"'s\", 'friend']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "list(map(str, tokenizer(\"\"\"hello meow. meow is donald trump's friend\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint '/Users/GuruSenthil/Desktop/pair-bert-train_wiki-val_wiki-5-1.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "# loading from the model checkpoint state\n",
    "\n",
    "model.eval()\n",
    "state_dict = __load_model__(checkpoint_path)['state_dict']\n",
    "own_state = model.state_dict()\n",
    "for name, param in state_dict.items():\n",
    "    if name not in own_state:\n",
    "        continue\n",
    "    own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on the wikidata dataset, which is what they have already implemented.\n",
    "\n",
    "N = 5\n",
    "K = 1\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "with torch.no_grad():\n",
    "    for it in range(10):\n",
    "        batch, label = next(val_data_loader)\n",
    "        label = torch.tensor(label)\n",
    "        batch['word'] = torch.stack(batch['word'])\n",
    "        batch['seg'] = torch.stack(batch['seg'])\n",
    "        batch['mask'] = torch.stack(batch['mask'])\n",
    "        logits, pred = model(batch, N, K, Q * N + Q * na_rate)\n",
    "        print(pred, label)\n",
    "        right = model.accuracy(pred, label)\n",
    "        print(item(right.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "K = 2\n",
    "Q = 1\n",
    "na_rate = 0\n",
    "example_relation_data = [\n",
    "    {'name':'love',\n",
    "    'examples':[\n",
    "        {'sentence':'meow loves mo', 'head':'meow', 'tail':'mo'},\n",
    "        {'sentence':'tom is in love with jull', 'head':'tom', 'tail':'jull'}\n",
    "    ]},\n",
    "    {'name':'hate',\n",
    "    'examples':[\n",
    "        {'sentence':'trump hates the mooch', 'head':'trump', 'tail':'mooch'},\n",
    "        {'sentence':'ivanka and jared dislike each other intensely', 'head':'ivanka', 'tail':'jared'}\n",
    "    ]},\n",
    "    {'name':'spouse',\n",
    "    'examples':[\n",
    "        {'sentence':'trump is married to ivanka', 'head':'trump', 'tail':'ivanka'},\n",
    "        {'sentence':\"bill went out with his wife jill on saturday\", 'head':'bill', 'tail':'jill'}\n",
    "    ]},\n",
    "        {'name':'insult',\n",
    "    'examples':[\n",
    "        {'sentence':'The president said that michael cohen is a rat', 'head':'The president', 'tail':'michael'},\n",
    "        {'sentence':'meow and tom threw jabs at each other', 'head':'meow', 'tail':'tom'}\n",
    "    ]},\n",
    "        {'name':'capital',\n",
    "    'examples':[\n",
    "        {'sentence':'austin is the capital of texas', 'head':'austin', 'tail':'texas'},\n",
    "        {'sentence':\"the capital of china is located in beijing\", 'head':'beijing', 'tail':\"china\"}\n",
    "    ]}\n",
    "    \n",
    "]\n",
    "\n",
    "queries = [{\n",
    "    'sentence':'furball and fluffy are very loving to each other','head':'furball','tail':'fluffy'\n",
    "},\n",
    "{\n",
    "    'sentence':\"\"\"US's capital is washington\"\"\",'head':'washington','tail':'US'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([[[ 6.0336,  5.4629,  4.9938,  2.5218, -3.1381, -4.6057]]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([4]) tensor([[[-3.2909, -2.9994, -3.9807, -2.8774,  5.9306, -4.5772]]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_tokenize(sentence):\n",
    "    return list(map(str, nlp(sentence)))\n",
    "\n",
    "max_length = 128\n",
    "for q in queries:\n",
    "    fusion_set = {'word': [], 'mask': [], 'seg': []}\n",
    "#     tokens = q['sentence'].split(\" \")  #TODO: generalize, make it tokenize like in the example wikidata, would probably need to use some nlp library to do it\n",
    "    tokens = spacy_tokenize(q['sentence'])\n",
    "    tokenized_head = spacy_tokenize(q['head'])\n",
    "    tokenized_tail = spacy_tokenize(q['tail'])\n",
    "    head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))   #TODO: make it work with multi-word entities\n",
    "    tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "    bert_query_tokens = tokenize(tokens, head_indices, tail_indices)\n",
    "    for relation in example_relation_data:\n",
    "        for ex in relation['examples']:\n",
    "#             tokens = ex['sentence'].split(\" \")  #TODO: generalize\n",
    "            tokens = spacy_tokenize(ex['sentence'])\n",
    "            tokenized_head = spacy_tokenize(ex['head'])\n",
    "            tokenized_tail = spacy_tokenize(ex['tail'])\n",
    "            head_indices = list(range(tokens.index(tokenized_head[0]), tokens.index(tokenized_head[0])+len(tokenized_head)))\n",
    "            tail_indices = list(range(tokens.index(tokenized_tail[0]), tokens.index(tokenized_tail[0])+len(tokenized_tail)))\n",
    "            bert_relation_example_tokens = tokenize(tokens, head_indices, tail_indices)\n",
    "            \n",
    "            SEP = sentence_encoder.tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            CLS = sentence_encoder.tokenizer.convert_tokens_to_ids(['[CLS]'])\n",
    "            word_tensor = torch.zeros((max_length)).long()\n",
    "            \n",
    "            new_word = CLS + bert_relation_example_tokens + SEP + bert_query_tokens + SEP\n",
    "            for i in range(min(max_length, len(new_word))):\n",
    "                word_tensor[i] = new_word[i]\n",
    "            mask_tensor = torch.zeros((max_length)).long()\n",
    "            mask_tensor[:min(max_length, len(new_word))] = 1\n",
    "            seg_tensor = torch.ones((max_length)).long()\n",
    "            seg_tensor[:min(max_length, len(bert_relation_example_tokens) + 1)] = 0\n",
    "            fusion_set['word'].append(word_tensor)\n",
    "            fusion_set['mask'].append(mask_tensor)\n",
    "            fusion_set['seg'].append(seg_tensor)\n",
    "    \n",
    "    fusion_set['word'] = torch.stack(fusion_set['word'])\n",
    "    fusion_set['seg'] = torch.stack(fusion_set['seg'])\n",
    "    fusion_set['mask'] = torch.stack(fusion_set['mask'])\n",
    "    logits, pred = model(fusion_set, N, K, 1)\n",
    "    print(pred, logits)\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David is a cool boy, David went to narnia on saturday, and Sally is great, Sally played with David yesterday.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# ex = \"hello Meow. Meow is Donald Trump's friend\"\n",
    "ex = \"David is a cool boy, he went to narnia on saturday, and Sally is great, She played with me yesterday.\"\n",
    "doc = nlp(ex)\n",
    "print(doc._.coref_resolved)\n",
    "doc = nlp(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David', 'PERSON'), ('David', 'PERSON'), ('saturday', 'DATE'), ('Sally', 'PERSON'), ('Sally', 'PERSON'), ('David', 'PERSON'), ('yesterday', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cool',\n",
       " 'boy',\n",
       " ',',\n",
       " 'David',\n",
       " 'went',\n",
       " 'to',\n",
       " 'narnia',\n",
       " 'on',\n",
       " 'saturday',\n",
       " ',',\n",
       " 'and',\n",
       " 'Sally',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'Sally',\n",
       " 'played',\n",
       " 'with',\n",
       " 'David',\n",
       " 'yesterday',\n",
       " '.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
