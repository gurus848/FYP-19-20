{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing OpenNRE\n",
    "\n",
    "* Followed the instructions on this github to install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre\n",
    "import os\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0109 18:13:51.323661 140734763410880 bert_encoder.py:19] Loading BERT pre-trained checkpoint.\n",
      "I0109 18:13:51.325680 140734763410880 configuration_utils.py:182] loading configuration file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/config.json\n",
      "I0109 18:13:51.326586 140734763410880 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0109 18:13:51.329145 140734763410880 modeling_utils.py:403] loading weights file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/pytorch_model.bin\n",
      "I0109 18:13:53.603092 140734763410880 tokenization_utils.py:327] Model name '/Users/GuruSenthil/.opennre/pretrain/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/Users/GuruSenthil/.opennre/pretrain/bert-base-uncased' is a path or url to a directory containing tokenizer files.\n",
      "I0109 18:13:53.604340 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "I0109 18:13:53.605220 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/special_tokens_map.json. We won't load it.\n",
      "I0109 18:13:53.606104 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/tokenizer_config.json. We won't load it.\n",
      "I0109 18:13:53.606884 140734763410880 tokenization_utils.py:395] loading file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/vocab.txt\n",
      "I0109 18:13:53.607397 140734763410880 tokenization_utils.py:395] loading file None\n",
      "I0109 18:13:53.607920 140734763410880 tokenization_utils.py:395] loading file None\n",
      "I0109 18:13:53.608361 140734763410880 tokenization_utils.py:395] loading file None\n"
     ]
    }
   ],
   "source": [
    "model = opennre.get_model('wiki80_bert_softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('father', 0.9925805330276489)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer({'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0109 19:55:25.713931 140734763410880 bert_encoder.py:19] Loading BERT pre-trained checkpoint.\n",
      "I0109 19:55:25.715536 140734763410880 configuration_utils.py:182] loading configuration file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/config.json\n",
      "I0109 19:55:25.716428 140734763410880 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0109 19:55:25.717285 140734763410880 modeling_utils.py:403] loading weights file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/pytorch_model.bin\n",
      "I0109 19:55:27.897793 140734763410880 tokenization_utils.py:327] Model name '/Users/GuruSenthil/.opennre/pretrain/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/Users/GuruSenthil/.opennre/pretrain/bert-base-uncased' is a path or url to a directory containing tokenizer files.\n",
      "I0109 19:55:27.898931 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "I0109 19:55:27.900017 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/special_tokens_map.json. We won't load it.\n",
      "I0109 19:55:27.901046 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/tokenizer_config.json. We won't load it.\n",
      "I0109 19:55:27.901748 140734763410880 tokenization_utils.py:395] loading file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/vocab.txt\n",
      "I0109 19:55:27.902309 140734763410880 tokenization_utils.py:395] loading file None\n",
      "I0109 19:55:27.902853 140734763410880 tokenization_utils.py:395] loading file None\n",
      "I0109 19:55:27.903577 140734763410880 tokenization_utils.py:395] loading file None\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder = opennre.encoder.BERTEncoder(\n",
    "            max_length=80, pretrain_path=os.path.join('/Users/GuruSenthil/.opennre', 'pretrain/bert-base-uncased'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sentence_encoder.tokenize({'text': 'austin is the capital of texas', 'h': {'pos': (0, 6)}, 't': {'pos': (25, 30)}})\n",
    "a = sentence_encoder(*items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sentence_encoder.tokenize({'text': 'meow is the capital of cat', 'h': {'pos': (0, 4)}, 't': {'pos': (23, 26)}})\n",
    "b = sentence_encoder(*items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sentence_encoder.tokenize({'text': \"meow is donald trump's friend. and meow is a cat.\", 'h': {'pos': (0, 4)}, 't': {'pos': (8, 20)}})\n",
    "c = sentence_encoder(*items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9665], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "cos(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8142], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0109 21:21:30.306677 140734763410880 bert_encoder.py:19] Loading BERT pre-trained checkpoint.\n",
      "I0109 21:21:30.309437 140734763410880 configuration_utils.py:182] loading configuration file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/config.json\n",
      "I0109 21:21:30.310675 140734763410880 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0109 21:21:30.312001 140734763410880 modeling_utils.py:403] loading weights file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/pytorch_model.bin\n",
      "I0109 21:21:32.291840 140734763410880 tokenization_utils.py:327] Model name '/Users/GuruSenthil/.opennre/pretrain/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/Users/GuruSenthil/.opennre/pretrain/bert-base-uncased' is a path or url to a directory containing tokenizer files.\n",
      "I0109 21:21:32.292820 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "I0109 21:21:32.293312 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/special_tokens_map.json. We won't load it.\n",
      "I0109 21:21:32.295418 140734763410880 tokenization_utils.py:359] Didn't find file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/tokenizer_config.json. We won't load it.\n",
      "I0109 21:21:32.296122 140734763410880 tokenization_utils.py:395] loading file /Users/GuruSenthil/.opennre/pretrain/bert-base-uncased/vocab.txt\n",
      "I0109 21:21:32.296607 140734763410880 tokenization_utils.py:395] loading file None\n",
      "I0109 21:21:32.297194 140734763410880 tokenization_utils.py:395] loading file None\n",
      "I0109 21:21:32.297756 140734763410880 tokenization_utils.py:395] loading file None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = \"/Users/GuruSenthil/.opennre\"\n",
    "ckpt = '/Users/GuruSenthil/.opennre/pretrain/nre/wiki80_bert_softmax.pth.tar'\n",
    "rel2id = json.load(open(os.path.join(root_path, 'benchmark/wiki80/wiki80_rel2id.json')))\n",
    "sentence_encoder = opennre.encoder.BERTEncoder(\n",
    "    max_length=80, pretrain_path=os.path.join(root_path, 'pretrain/bert-base-uncased'))\n",
    "m = opennre.model.SoftmaxNN(sentence_encoder, len(rel2id), rel2id)\n",
    "m.load_state_dict(torch.load(ckpt, map_location='cpu')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3427, -0.1440, -1.3654, -0.4375, -1.7786, -0.5298,  1.9023, -0.6984,\n",
       "         -2.8205, -0.3486,  0.0706,  0.5511,  0.9234,  0.5459, -2.2748, -2.1622,\n",
       "         -2.1507,  0.2960, -0.4731,  2.4915,  1.0260,  0.8903, -0.8175, -0.7492,\n",
       "         -0.9382, -2.5027,  0.2545,  1.1382, -0.9117,  1.7248, -0.2244, -0.8402,\n",
       "         -0.0466,  2.2004, -0.9679,  0.8496,  1.3005,  1.5516, -0.2336, -0.1541,\n",
       "          1.9099,  1.9824,  4.8553,  0.9758, -4.1883, -1.1187,  5.0358, -2.7919,\n",
       "         -0.7149, -0.1335,  2.2405, -2.0716,  0.4516, -2.3358, -0.0340,  1.0407,\n",
       "          1.4622,  0.6326,  0.5264, -0.4178, -0.3764,  0.3265, -1.1930, -0.1548,\n",
       "          0.9341, -0.4265, -0.6686,  0.9628,  0.0769, -0.2006, -0.8531, -2.1909,\n",
       "         -0.6461, -1.2162,  2.6883, -0.4939,  1.0412,  1.7167, -0.8559, -0.6045]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = sentence_encoder.tokenize({'text': \"meow is donald trump's friend. and meow is a cat.\", 'h': {'pos': (0, 4)}, 't': {'pos': (8, 20)}})\n",
    "a = m(*items)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1560, -0.1905, -0.6039, -2.9801, -1.7388,  0.5615,  0.5146, -0.7127,\n",
       "         -0.6256, -1.5320, -1.1279,  0.6374, -0.9176,  6.5828, -0.5549,  0.1322,\n",
       "          2.1347,  1.8245, -2.4338, -0.7260,  1.2941,  0.3351, -0.3702, -2.7036,\n",
       "         -0.1676, -2.3188, -1.0749, -1.0955, -2.6997, -1.3087, -0.5501,  1.5541,\n",
       "          9.0482, -0.5609, -0.4973, -0.0460, -0.0277,  3.0687,  2.3323,  1.0484,\n",
       "         -1.4314, -0.2442, -0.7876,  1.0941, -1.8151, -1.5772,  3.0612, -0.5493,\n",
       "         -2.5068, -1.8567, -0.2186, -0.8552,  1.5331, -0.2951,  0.0881, -1.4775,\n",
       "         -1.1968, -1.5754,  3.9728, -0.8819,  0.2813, -0.3515, -2.7493,  3.3888,\n",
       "          0.5984, -1.0252, -0.7704, -0.8025,  0.1246, -0.3000,  0.3585, -0.7857,\n",
       "         -1.4500, -0.9740, -1.5255, -0.2581,  0.2017,  0.3956,  0.2308, -1.5301]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = sentence_encoder.tokenize({'text': 'meow is the capital of cat', 'h': {'pos': (0, 4)}, 't': {'pos': (23, 26)}})\n",
    "b = m(*items)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0487e+00,  5.0125e-01, -1.2646e+00, -2.7185e+00,  3.2718e+00,\n",
       "         -9.7491e-01, -3.3797e+00,  2.8477e+00,  4.7379e-01, -3.3002e-01,\n",
       "         -1.4340e+00, -2.7140e+00,  2.9508e-03, -9.0510e-01,  2.1070e+00,\n",
       "         -3.1967e-01, -1.0099e+00, -1.4511e+00, -3.8937e-04, -1.7084e+00,\n",
       "         -7.5802e-01, -2.9236e-01,  1.5564e-01, -2.4067e-01, -2.5359e+00,\n",
       "         -1.4531e+00,  2.1920e-01, -3.6974e-01,  1.4170e+00, -1.1669e+00,\n",
       "         -1.5002e+00,  2.0625e-01,  2.6768e+00, -1.7314e+00, -3.9138e+00,\n",
       "          2.5759e+00, -1.3503e+00,  1.9748e+00,  2.3183e-01, -2.1805e+00,\n",
       "         -2.9977e+00,  5.3179e-01,  4.7085e-01,  7.9911e-01, -1.5058e+00,\n",
       "         -1.0703e+00,  2.7962e+00,  1.1350e+00,  7.6377e-01, -2.2252e+00,\n",
       "         -1.6672e+00, -5.4009e-02,  1.1117e+00, -1.9170e+00,  9.6382e-01,\n",
       "         -1.0127e+00, -5.8511e-02, -1.5312e+00,  3.7789e+00, -1.7765e+00,\n",
       "         -1.0639e-01,  9.6908e-01, -5.1674e-01,  1.1604e+01,  6.5263e-01,\n",
       "          4.5381e-01, -2.3814e+00,  2.3448e+00, -8.5856e-01, -1.4667e+00,\n",
       "          8.1213e-01, -1.3621e+00, -1.4853e+00,  5.7614e-01,  6.4172e-01,\n",
       "         -7.4475e-01, -1.3113e+00, -3.9396e+00,  1.0943e+00,  4.5050e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = sentence_encoder.tokenize({'text': 'austin is the capital of texas', 'h': {'pos': (0, 6)}, 't': {'pos': (25, 30)}})\n",
    "c = m(*items)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1791], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3375], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
